{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b71d48-b8e8-400d-ac64-5f648c2b9c7b",
      "metadata": {
        "id": "b9b71d48-b8e8-400d-ac64-5f648c2b9c7b"
      },
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Define the connection details\n",
        "uri =       \"neo4j://localhost:7687\"\n",
        "username =  \"neo4j\"\n",
        "password =  \"password\"\n",
        "\n",
        "\n",
        "# Connect to the Neo4j database\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78e6c317-dea7-449c-ab4c-58eb4bb2b168",
      "metadata": {
        "id": "78e6c317-dea7-449c-ab4c-58eb4bb2b168"
      },
      "source": [
        "# figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ee6281-be63-4173-b2b6-0ee8963a290a",
      "metadata": {
        "id": "b5ee6281-be63-4173-b2b6-0ee8963a290a",
        "outputId": "cc5248b7-3cbf-4ece-b321-ca3844242218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: python-louvain in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python313\\site-packages (0.16)\n",
            "Requirement already satisfied: networkx in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python313\\site-packages (from python-louvain) (3.4.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python313\\site-packages (from python-louvain) (2.2.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "!pip install python-louvain\n",
        "import networkx as nx\n",
        "import community\n",
        "from community import community_louvain\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "layout_algorithms = [\n",
        "    nx.circular_layout,\n",
        "    nx.random_layout,\n",
        "    nx.spring_layout,\n",
        "    nx.spectral_layout,\n",
        "    nx.shell_layout,\n",
        "    nx.bipartite_layout,\n",
        "    nx.planar_layout,\n",
        "    nx.fruchterman_reingold_layout,\n",
        "    nx.kamada_kawai_layout\n",
        "]\n",
        "\n",
        "A=['PGM1-CDG','Cerebral cavernous malformations 3',\"CLOVES syndrome\",\"STT3B-CDG\",\"Thoracolaryngopelvic dysplasia\",\"Actinic prurigo\",\"Non-syndromic polydactyly\",\"Genitopatellar syndrome\",\"Diaphanospondylodysostosis\"]\n",
        "B=['GARD0004329','GARD0018314',\"GARD0010939\",\"GARD0017603\",\"GARD0005184\", \"GARD0017510\",\"GARD0004410\",\"GARD0010994\",\"GARD0016674\"]\n",
        "for n in range(len(A)):\n",
        "  disease= A[n]  #'Actinic prurigo' #'Non-syndromic polydactyly'   #'Cystic fibrosis'\n",
        "  diseaseid=B[n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e07885-9470-43c5-b18a-0ed390fc2bf1",
      "metadata": {
        "id": "71e07885-9470-43c5-b18a-0ed390fc2bf1"
      },
      "outputs": [],
      "source": [
        "def get_reseracher_info(disease):\n",
        " with driver.session() as session:\n",
        "  query = \"MATCH (i:Researcher)--(C:Cluster)--(g:Gard) WHERE g.GARDname=$disease OPTIONAL MATCH (i)-[:Collaborated_With]-(n:Researcher)  WITH i, COLLECT(n.Name + ', @institution: ' + n.Aff_name) AS collaborators RETURN i.Name + ', @institution: ' + i.Aff_name AS researcher, collaborators \"\n",
        "  results = session.run(query, disease=disease)\n",
        "  data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "  reseracher_info = pd.DataFrame(data)\n",
        " return reseracher_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "babfd67f-f297-4757-9080-4b01aa013886",
      "metadata": {
        "id": "babfd67f-f297-4757-9080-4b01aa013886"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "disease='Cystic fibrosis'\n",
        "#cluster_info=get_cluster(disease)\n",
        "reseracher_info=get_reseracher_info(disease)\n",
        "#expertise=get_expertise_info(disease)\n",
        "#cluster_info\n",
        "#reseracher_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5c958f0-b97e-45a5-8e34-892ee3cc7c81",
      "metadata": {
        "id": "b5c958f0-b97e-45a5-8e34-892ee3cc7c81",
        "outputId": "02a56b5e-1483-43fa-ef2e-267f060219c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def draw_graph(disease, HA=\"center\", VA=\"center\"):\n",
        " #cluster_info=get_cluster(disease)\n",
        " #reseracher_info=get_reseracher_info(disease)\n",
        " #expertise=get_expertise_info(disease)\n",
        "\n",
        " # Create a directed graph\n",
        " G = nx.Graph()\n",
        " reseracher_info1=reseracher_info[:2000]\n",
        " for i in reseracher_info1.index:\n",
        "    new_node = reseracher_info1['researcher'][i]\n",
        "    if new_node not in G:\n",
        "        G.add_node(new_node)\n",
        "    collab = reseracher_info1['collaborators'][i]\n",
        "    for edge in collab:\n",
        "        if edge not in G:\n",
        "            G.add_node(edge)\n",
        "        if not G.has_edge(new_node, edge):\n",
        "            G.add_edge(new_node, edge)\n",
        "\n",
        " #partition = cluster_info\n",
        " #nodes_to_remove = [node for node, attr in G.nodes(data=True) if node not in cluster_info]\n",
        " #for node in nodes_to_remove:\n",
        " #   if node in G:\n",
        " #       G.remove_node(node)\n",
        "\n",
        " # Draw the graph with nodes colored by their community\n",
        " #pos = nx.kamada_kawai_layout(G)\n",
        " #cmap = plt.cm.get_cmap('viridis', max(partition.values()) + 1)\n",
        "\n",
        " partition = community_louvain.best_partition(G)\n",
        " #print(partition)\n",
        " pos = nx.kamada_kawai_layout(G)\n",
        " cmap = plt.cm.get_cmap('viridis', max(partition.values()) + 1)\n",
        " node_colors = [cmap(partition[node]) for node in G.nodes()]\n",
        " nx.draw_networkx_nodes(G, pos, node_size=20, node_color=node_colors,  alpha=0.8)\n",
        "\n",
        "\n",
        " #pos = nx.spring_layout(G, k=0.1, iterations=50, seed=42)\n",
        " #cmap = plt.cm.get_cmap('viridis', max(partition.values()) + 1)\n",
        " #node_colors = [cmap(partition[node]) for node in G.nodes()]\n",
        " #nx.draw_networkx_nodes(G, pos, node_size=20, node_color=node_colors, alpha=0.8)\n",
        "\n",
        "\n",
        " #nx.draw_networkx_nodes(G, pos, node_size=20, node_color=[cmap(partition[node]) for node in G.nodes()], alpha=0.8)\n",
        " nx.draw_networkx_edges(G, pos, edge_color=\"gray\", width=0.2, alpha=0.6)\n",
        " '''\n",
        " # Label one node per community\n",
        " community_labels = {}\n",
        " for node, community_id in partition.items():\n",
        "    if community_id not in community_labels:\n",
        "        community_labels[community_id] = node\n",
        "\n",
        " # Set figure limits\n",
        " x_vals, y_vals = zip(*pos.values())\n",
        " x_min, x_max = min(x_vals), max(x_vals)\n",
        " y_min, y_max = min(y_vals), max(y_vals)\n",
        " x_margin = (x_max - x_min)*0.1 #* 0.3\n",
        " y_margin = (y_max - y_min)*0.1 #* 0.3\n",
        "\n",
        " #ax.set_frame_on(False)  # Disable figure border\n",
        " plt.axis('off')\n",
        " # Organize labels to avoid overlaps\n",
        " for community_id, node in community_labels.items():\n",
        "  #if community_id>1:\n",
        "    x, y = pos[node]\n",
        "\n",
        "    # Apply an offset for the labels\n",
        "    offset = 0.1 + 0.2 * community_id\n",
        "    x_label = x + offset * np.cos(community_id)\n",
        "    y_label = y + offset * np.sin(community_id)\n",
        "\n",
        "    # Clamp label positions within the figure bounds\n",
        "    x_label = max(x_min - x_margin, min(x_max + x_margin, x_label))\n",
        "    y_label = max(y_min - y_margin, min(y_max + y_margin, y_label))\n",
        "\n",
        "    # Display label with expertise information\n",
        "    try:\n",
        "        label_text = (\n",
        "            f\"C{community_id}: {expertise[community_id][:25]}\\n\"\n",
        "            f\"{expertise[community_id][25:50]}\\n\"\n",
        "            f\"{expertise[community_id][50:75]}\\n\"\n",
        "            f\"{expertise[community_id][75:100]}\\n\"\n",
        "            f\"{expertise[community_id][100:125]}...\"\n",
        "        )\n",
        "        plt.text(\n",
        "            x_label, y_label, label_text, fontsize=6, color=\"green\", ha=HA, va=VA,\n",
        "            bbox=dict(facecolor=\"white\", alpha=0.6, edgecolor=\"red\", boxstyle=\"round,pad=0.3\")\n",
        "        )\n",
        "    except:\n",
        "        pass\n",
        " '''\n",
        " # Finalize plot\n",
        " #plt.title(\"Social Network for Rare Disease-Based Research Collaboration Recommendation\")\n",
        " plt.xlim(x_min - x_margin, x_max + x_margin)\n",
        " plt.ylim(y_min - y_margin, y_max + y_margin)\n",
        " #plt.xlim(x_min, x_max )\n",
        " #plt.ylim(y_min , y_max )\n",
        " plt.show()\n",
        "draw_graph('Cystic fibrosis', HA=\"right\", VA=\"center\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "809ca0bd-f96a-4541-b25d-33f35351ae43",
      "metadata": {
        "id": "809ca0bd-f96a-4541-b25d-33f35351ae43"
      },
      "source": [
        "CALL db.schema.visualization()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db639bd9-26fe-4614-bff7-a77887a683b1",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "db639bd9-26fe-4614-bff7-a77887a683b1"
      },
      "source": [
        "# First part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d683a98d-ee2b-4552-8403-17a0a952d963",
      "metadata": {
        "id": "d683a98d-ee2b-4552-8403-17a0a952d963"
      },
      "outputs": [],
      "source": [
        "# Define node types\n",
        "node_types = ['Researcher',\n",
        "'Cluster',\n",
        "'Location',\n",
        "'Expertise',\n",
        "'Gard',\n",
        "'ClinicalTrial',\n",
        "'Grant',\n",
        "'Pubmed']\n",
        "\n",
        "# Create nodes\n",
        "with driver.session() as session:\n",
        "    for node_type in node_types:\n",
        "        query = f\"CREATE (:`{node_type}`)\"\n",
        "        session.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53397367-b4aa-472f-8529-af8e89e85d90",
      "metadata": {
        "id": "53397367-b4aa-472f-8529-af8e89e85d90",
        "outputId": "eae57335-bb07-4c67-b87e-1d5b678aba42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1617 14033 215248\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "#ClinicalTrial = pd.read_csv('C:/Users/Desktop/Social network all works/work rdas/Clinicaltrial.csv')\n",
        "#Grant = pd.read_csv('C:/Users/Desktop/Social network all works/work rdas/Grant.csv')#,  error_bad_lines=False)\n",
        "#Pubmed = pd.read_csv('C:/Users/Desktop/Social network all works/work rdas/Pubmed.csv', on_bad_lines='skip')# on_bad_lines='skip') #error_bad_lines=False\n",
        "\n",
        "ClinicalTrial = pd.read_csv('C:/Users/valinejadj2/Desktop/ClinicalTrial.csv')\n",
        "Grant = pd.read_csv('C:/Users/Desktop/Grants.csv')#,  error_bad_lines=False)\n",
        "data1=pd.read_csv('C:/Users/Desktop/pubmed1.csv')\n",
        "data2=pd.read_csv('C:/Users/Desktop/pubmed2.csv')\n",
        "data3=pd.read_csv('C:/Users/Desktop/pubmed3.csv')\n",
        "data4=pd.read_csv('C:/Users/Desktop/Pubmed.csv')\n",
        "data=pd.concat([data1,data2,data3,data4], ignore_index=True)\n",
        "Pubmed = data.drop_duplicates()\n",
        "Pubmed.index= range(Pubmed.shape[0])\n",
        "ClinicalTrial = ClinicalTrial.replace('\"', '', regex=True)\n",
        "Grant = Grant.replace('\"', '', regex=True)\n",
        "Pubmed = Pubmed.replace('\"', '', regex=True)\n",
        "print(ClinicalTrial.shape[0], Grant.shape[0], Pubmed.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35c6e2d-e684-4386-b7a5-9f11bd99d7f2",
      "metadata": {
        "id": "c35c6e2d-e684-4386-b7a5-9f11bd99d7f2",
        "outputId": "b1bf077f-4666-4443-9fdb-4aa2291418d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nulls in 'c.BriefSummary': 9\n",
            "Number of empty strings in 'c.BriefSummary': 14945\n"
          ]
        }
      ],
      "source": [
        "num_nulls = Pubmed['Abstract'].isnull().sum()\n",
        "print(f\"Number of nulls in 'c.BriefSummary': {num_nulls}\")\n",
        "num_empty_strings = (Pubmed['Abstract'] == '').sum()\n",
        "print(f\"Number of empty strings in 'c.BriefSummary': {num_empty_strings}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94189200-1695-4ac0-8d39-5f5ee5ae74ea",
      "metadata": {
        "scrolled": true,
        "id": "94189200-1695-4ac0-8d39-5f5ee5ae74ea",
        "outputId": "3526c693-e2af-4f5c-a9a3-30dc32b671a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['i.OfficialName', 'i.OfficialAffiliation', 'c. BriefTitle',\n",
            "       'c.OfficialTitle', 'c.BriefSummary', 'c.NCTId', 'l.LocationCity',\n",
            "       'l. LocationState', 'l.LocationCountry', 'l.LocationFacility',\n",
            "       'Interventions'],\n",
            "      dtype='object')\n",
            "Index(['pi.pi_name', 'pi.org_name', 'p.title', 'p.application_id',\n",
            "       'p.abstract', 'collect(distinct p.terms)'],\n",
            "      dtype='object')\n",
            "Index(['Title', 'PubMedID', 'Abstract', 'Keywords', 'AuthorFullName',\n",
            "       'AuthorAffiliation'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(ClinicalTrial.columns)\n",
        "print(Grant.columns)\n",
        "print(Pubmed.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448584fd-e971-4b16-b056-5ac654d4e69e",
      "metadata": {
        "id": "448584fd-e971-4b16-b056-5ac654d4e69e"
      },
      "outputs": [],
      "source": [
        "ClinicalTrial['GARDname']= 'Cystic fibrosis'\n",
        "Grant['GARDname']= 'Cystic fibrosis'\n",
        "Pubmed['GARDname']= 'Cystic fibrosis'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df8e9148-f6a0-460d-a412-07ad138238df",
      "metadata": {
        "id": "df8e9148-f6a0-460d-a412-07ad138238df",
        "outputId": "25e1b677-c3eb-4be0-d2c7-566c0986be05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DF1 columns: Index(['OfficialName', 'OfficialAffiliation', 'BriefTitle', 'OfficialTitle',\n",
            "       'BriefSummary', 'NCTId', 'LocationCity', 'LocationState',\n",
            "       'LocationCountry', 'LocationFacility', 'Interventions', 'GARDname'],\n",
            "      dtype='object')\n",
            "DF2 columns: Index(['pi_name', 'org_name', 'title', 'application_id', 'abstract', 'terms',\n",
            "       'GARDname'],\n",
            "      dtype='object')\n",
            "DF3 columns: Index(['title', 'pubmed_id', 'abstractText', 'concatenatedKeywords',\n",
            "       'fullName', 'affiliation', 'GARDname'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "rename_dict_1 = {\n",
        "    'i.OfficialName':         'OfficialName',\n",
        "    'i.OfficialAffiliation':  'OfficialAffiliation',\n",
        "    'c. BriefTitle':       'BriefTitle',\n",
        "    'c.OfficialTitle':    'OfficialTitle',\n",
        "    'c.BriefSummary':     'BriefSummary',\n",
        "    'c.NCTId':            'NCTId',\n",
        "    'l.LocationCity':     'LocationCity',\n",
        "    'l. LocationState':   'LocationState',\n",
        "    'l.LocationCountry':  'LocationCountry',\n",
        "    'l.LocationFacility': 'LocationFacility' }\n",
        "\n",
        "rename_dict_2 = {\n",
        "    'pi.pi_name':           'pi_name',\n",
        "    'pi.org_name':          'org_name',\n",
        "    'p.title':              'title',\n",
        "    'p.application_id':     'application_id',\n",
        "    'p.abstract':           'abstract',\n",
        "    'collect(distinct p.terms)':              'terms'\n",
        "}\n",
        "\n",
        "rename_dict_3 = {\n",
        "    'AuthorFullName':            'fullName'        ,\n",
        "    'Title':              'title'       ,\n",
        "    'AuthorAffiliation':        'affiliation'       ,\n",
        "    'PubMedID':          'pubmed_id'      ,\n",
        "    'Abstract':       'abstractText'     ,\n",
        "    'Keywords':  'concatenatedKeywords'\n",
        "}\n",
        "\n",
        "\n",
        "# Rename the columns of each DataFrame\n",
        "ClinicalTrial.rename(columns=rename_dict_1, inplace=True)\n",
        "Grant.rename(columns=rename_dict_2, inplace=True)\n",
        "Pubmed.rename(columns=rename_dict_3, inplace=True)\n",
        "\n",
        "# Check the renamed columns\n",
        "print(\"DF1 columns:\", ClinicalTrial.columns)\n",
        "print(\"DF2 columns:\", Grant.columns)\n",
        "print(\"DF3 columns:\", Pubmed.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b1cf68-cb41-43a9-b58f-1a38bb516477",
      "metadata": {
        "id": "69b1cf68-cb41-43a9-b58f-1a38bb516477"
      },
      "outputs": [],
      "source": [
        "def create_Gard_node(properties):\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            query = (\n",
        "                \"create (ga:Gard {\"\n",
        "                'GARDID: $GARDID,'\n",
        "                'GARDname: $GARDname'\n",
        "                \"})\"\n",
        "            )\n",
        "            session.run(query, **properties)\n",
        "def create_ClinicalTrial_node(properties):\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            query = (\n",
        "                \"create (CT:ClinicalTrial {\"\n",
        "                'OfficialName: $OfficialName,'\n",
        "                'OfficialAffiliation: $OfficialAffiliation,'\n",
        "                'BriefTitle: $BriefTitle,'\n",
        "                'OfficialTitle: $OfficialTitle,'\n",
        "                'BriefSummary: $BriefSummary,'\n",
        "                'NCTId: $NCTId,'\n",
        "                'LocationCity: $LocationCity,'\n",
        "                'LocationState: $LocationState,'\n",
        "                'LocationCountry: $LocationCountry,'\n",
        "                'LocationFacility: $LocationFacility,'\n",
        "                'Interventions: $Interventions,'\n",
        "                'GARDname: $GARDname'\n",
        "                \"})\"\n",
        "            )\n",
        "            session.run(query, **properties)\n",
        "'''\n",
        "def create_ClinicalTrial_node(properties):\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            query = (\n",
        "                \"MERGE (CT:ClinicalTrial {\"\n",
        "                'NCTId: $NCTId'  # Unique identifier for the ClinicalTrial\n",
        "                \"}) \"\n",
        "                \"ON CREATE SET \"\n",
        "                'CT.OfficialName = $OfficialName, '\n",
        "                'CT.OfficialAffiliation = $OfficialAffiliation, '\n",
        "                'CT.BriefTitle = $BriefTitle, '\n",
        "                'CT.OfficialTitle = $OfficialTitle, '\n",
        "                'CT.BriefSummary = $BriefSummary, '\n",
        "                'CT.LocationCity = $LocationCity, '\n",
        "                'CT.LocationState = $LocationState, '\n",
        "                'CT.LocationCountry = $LocationCountry, '\n",
        "                'CT.LocationFacility = $LocationFacility, '\n",
        "                'CT.Interventions = $Interventions, '\n",
        "                'CT.GARDname = $GARDname'\n",
        "            )\n",
        "            session.run(query, **properties)\n",
        "'''\n",
        "def create_Grant_node(properties):\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            query = (\n",
        "                \"create (gr:Grant {\"\n",
        "    'pi_name: $pi_name,'\n",
        "    'org_name: $org_name,'\n",
        "    'title: $title,'\n",
        "    'application_id: $application_id,'\n",
        "    'abstract: $abstract,'\n",
        "    'terms: $terms,'\n",
        "    'GARDname: $GARDname'\n",
        "                \"})\"\n",
        "            )\n",
        "            session.run(query, **properties)\n",
        "def create_Pubmed_node(properties):\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            query = (\n",
        "                \"create (pm:Pubmed {\"\n",
        "    'fullName: $fullName,'\n",
        "    'ar_title: $ar_title,'\n",
        "    'ar_affiliation: $ar_affiliation,'\n",
        "    'pubmed_id: $pubmed_id,'\n",
        "    'abstractText: $abstractText,'\n",
        "    'concatenatedKeywords: $concatenatedKeywords,'\n",
        "    'GARDname: $GARDname'\n",
        "                \"})\"\n",
        "            )\n",
        "            session.run(query, **properties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fcbd7f8-b2f9-4b3f-9726-3fec8197c951",
      "metadata": {
        "id": "2fcbd7f8-b2f9-4b3f-9726-3fec8197c951"
      },
      "outputs": [],
      "source": [
        "for i in range(len(ClinicalTrial)):\n",
        "      event_properties = {\n",
        "                'OfficialName': ClinicalTrial['OfficialName'][i],\n",
        "                'OfficialAffiliation': ClinicalTrial['OfficialAffiliation'][i],\n",
        "                'BriefTitle': ClinicalTrial['BriefTitle'][i],\n",
        "                'OfficialTitle':ClinicalTrial['OfficialTitle'][i],\n",
        "                'BriefSummary': ClinicalTrial['BriefSummary'][i],\n",
        "                'NCTId': ClinicalTrial['NCTId'][i],\n",
        "                'LocationCity': ClinicalTrial['LocationCity'][i],\n",
        "                 'LocationState': ClinicalTrial['LocationState'][i],\n",
        "                'LocationCountry': ClinicalTrial['LocationCountry'][i],\n",
        "                'LocationFacility': ClinicalTrial['LocationFacility'][i],\n",
        "                'Interventions': ClinicalTrial['Interventions'][i],\n",
        "                'GARDname': ClinicalTrial['GARDname'][i]\n",
        "        }\n",
        "      create_ClinicalTrial_node(event_properties)\n",
        "print('CT: finished')\n",
        "\n",
        "for i in range(len(Grant)):\n",
        "    event_properties = {\n",
        "        'pi_name': str(Grant['pi_name'][i]),\n",
        "        'org_name': str(Grant['org_name'][i]),\n",
        "        'title': str(Grant['title'][i]),\n",
        "        'application_id': str(Grant['application_id'][i]),\n",
        "        'abstract': str(Grant['abstract'][i]),\n",
        "        'terms': str(Grant['terms'][i]),\n",
        "        'GARDname': Grant['GARDname'][i]\n",
        "    }\n",
        "    create_Grant_node(event_properties)\n",
        "print('Grant: finished')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1636dd00-72b3-4eae-814f-40333e49b228",
      "metadata": {
        "id": "1636dd00-72b3-4eae-814f-40333e49b228",
        "outputId": "a4b204ba-233a-4531-e333-6efbae76b7c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pubmed: finished\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(Pubmed)):\n",
        "    event_properties = {\n",
        "        'fullName': Pubmed['fullName'][i],  # Convert to string\n",
        "        'ar_title': Pubmed['title'][i],  # Convert to string\n",
        "        'ar_affiliation': Pubmed['affiliation'][i],  # Convert to string\n",
        "        'pubmed_id': Pubmed['pubmed_id'][i],  # Convert to string (or other appropriate type)\n",
        "        'abstractText': Pubmed['abstractText'][i],  # Convert to string\n",
        "        'concatenatedKeywords': Pubmed['concatenatedKeywords'][i],  # Convert to string\n",
        "        'GARDname': Pubmed['GARDname'][i]  # Convert to string\n",
        "    }\n",
        "    create_Pubmed_node(event_properties)\n",
        "\n",
        "print('Pubmed: finished')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55d4522-ee7e-44dc-aa2f-7fb5f0647a57",
      "metadata": {
        "id": "c55d4522-ee7e-44dc-aa2f-7fb5f0647a57"
      },
      "outputs": [],
      "source": [
        "def remove_duplicate_gard_nodes(uri, username, password):\n",
        "    query = \"\"\"\n",
        "    MATCH (ga:Gard)\n",
        "    WITH ga.GARDID AS id, COLLECT(ga) AS nodes\n",
        "    WHERE SIZE(nodes) > 1\n",
        "    FOREACH (n IN TAIL(nodes) | DELETE n)\n",
        "    \"\"\"\n",
        "\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            session.run(query)\n",
        "\n",
        "# Usage\n",
        "remove_duplicate_gard_nodes(uri, username, password)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d767d29-c1d5-41b7-a6a6-b7b4d4bbc9e4",
      "metadata": {
        "id": "5d767d29-c1d5-41b7-a6a6-b7b4d4bbc9e4"
      },
      "outputs": [],
      "source": [
        "def delete_all_gard_nodes():\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            session.run(\"MATCH (g:Gard) DETACH DELETE g\")\n",
        "\n",
        "delete_all_gard_nodes()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c849dbd-bea4-4972-94fe-08175aff4c12",
      "metadata": {
        "id": "2c849dbd-bea4-4972-94fe-08175aff4c12"
      },
      "outputs": [],
      "source": [
        "event_properties = {'GARDID': 'GARD0006233' ,'GARDname': 'Cystic fibrosis'}\n",
        "create_Gard_node(event_properties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "471c4c39-1b9d-4733-bf90-1d858a31bf9a",
      "metadata": {
        "id": "471c4c39-1b9d-4733-bf90-1d858a31bf9a"
      },
      "outputs": [],
      "source": [
        "def create_relationship_between_nodes(node1_label, node2_label, relationship_type):\n",
        "    with driver.session() as session:\n",
        "        query = (\n",
        "        f\"MATCH (n1:{node1_label}), (n2:{node2_label}) \"\n",
        "         f\"WHERE n1.GARDname = n2.GARDname \"\n",
        "       f\"CREATE (n1)-[:{relationship_type}]->(n2)\"\n",
        "        )\n",
        "        session.run(query)\n",
        "\n",
        "\n",
        "create_relationship_between_nodes('ClinicalTrial','Gard',  \"RELATED_GARD\")\n",
        "create_relationship_between_nodes('Grant','Gard',  \"RELATED_GARD\")\n",
        "create_relationship_between_nodes('Pubmed','Gard',  \"RELATED_GARD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b08494b6-987b-4e61-96d0-d625d45f89f5",
      "metadata": {
        "id": "b08494b6-987b-4e61-96d0-d625d45f89f5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "fips_finding=pd.read_csv('C:/Users/valinejadj2/Desktop/Social network all works/work rdas/uscities.csv')\n",
        "fips_finding_dict=dict()\n",
        "for i in fips_finding.index:\n",
        "     fips_finding_dict[(fips_finding['city'][i].lower().strip(),fips_finding['state_name'][i].lower().strip() )]= ['{:0>5}'.format(fips_finding['county_fips'][i]),fips_finding['county_name'][i]]\n",
        "     fips_finding_dict[fips_finding['city'][i].lower().strip()]= ['{:0>5}'.format(fips_finding['county_fips'][i]),fips_finding['county_name'][i]]\n",
        "\n",
        "def find_fip(x,y):\n",
        " if y != None:\n",
        "  try:\n",
        "    return (fips_finding_dict[ (x.lower().strip(),y.lower().strip())][0]), fips_finding_dict[ (x.lower(),y.lower())][1]\n",
        "  except:\n",
        "    return  '',''\n",
        " else:\n",
        "    try:\n",
        "      return (fips_finding_dict[x.lower().strip()][0]),fips_finding_dict[ x.lower()][1]\n",
        "    except:\n",
        "      return  '',''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32509b3f-d69b-47e8-9ce2-5a020206637d",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "32509b3f-d69b-47e8-9ce2-5a020206637d"
      },
      "outputs": [],
      "source": [
        "def create_Researcher_node(properties):\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            query = (\n",
        "                \"MERGE (Re:Researcher {\"\n",
        "                    'Name: $Name,'\n",
        "                    'Aff_name: $Aff_name'\n",
        "                \"}) \"\n",
        "                \"ON CREATE SET \"\n",
        "                    \"Re.Contact_info= $Contact_info,\"\n",
        "                    \"Re.Aff_Zip = $Aff_Zip \"\n",
        "\n",
        "            )\n",
        "            result = session.run(query, **properties)\n",
        "\n",
        "\n",
        "def create_Location_node(properties, Name, Aff_name):\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            query = (\n",
        "                \"MERGE (Loc:Location {Aff_country: $Aff_country, Aff_state: $Aff_state, Aff_county: $Aff_county}) \"\n",
        "                \"ON CREATE SET \"\n",
        "                \"Loc.Aff_city = $Aff_city, \"\n",
        "                \"Loc.Aff_FIPS = $Aff_FIPS \"\n",
        "                \"WITH Loc \"\n",
        "                \"MATCH (Re:Researcher {Name: $Name, Aff_name: $Aff_name}) \"\n",
        "                \"CREATE (Re)-[:Researcher_location]->(Loc)\"\n",
        "            )\n",
        "            # Execute the query, passing the properties dictionary and other parameters\n",
        "            result = session.run(query, **properties, Name=Name, Aff_name=Aff_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eea870e-d8ef-419b-bc21-b4980bee9647",
      "metadata": {
        "id": "3eea870e-d8ef-419b-bc21-b4980bee9647",
        "outputId": "df4140e8-f06a-493d-e224-c5f22d8c6383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: spacy in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (8.2.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (69.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.26.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy) (2.1.3)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ------- -------------------------------- 2.4/12.8 MB 13.4 MB/s eta 0:00:01\n",
            "     ---------------- ----------------------- 5.2/12.8 MB 13.3 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 7.3/12.8 MB 12.2 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.9/12.8 MB 11.1 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 10.2/12.8 MB 10.3 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 11.0/12.8 MB 9.2 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 11.8/12.8 MB 8.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.6/12.8 MB 7.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 7.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
            "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def classify_entity(text):\n",
        "    if not isinstance(text, str):  # Check if the input is a string\n",
        "        return False\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "display(classify_entity('Devon Leadman'))\n",
        "classify_entity('NIH')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b018b778-7e32-47ec-93d7-edecbf8724df",
      "metadata": {
        "id": "b018b778-7e32-47ec-93d7-edecbf8724df"
      },
      "outputs": [],
      "source": [
        "def extract_and_create_researcher_location_nodes_CT():\n",
        " with driver.session() as session:\n",
        "  query = \"MATCH (i:ClinicalTrial) RETURN i.OfficialName AS OfficialName, i.OfficialAffiliation AS OfficialAffiliation, i.LocationCountry as LocationCountry, i.LocationCity as LocationCity,i.LocationState as LocationState\"\n",
        "  results = session.run(query)\n",
        "  for ClinicalTrial in results:\n",
        "   Name=  ClinicalTrial['OfficialName']\n",
        "   Aff_name=   str(ClinicalTrial['OfficialAffiliation'])\n",
        "   if  classify_entity(Name)==True and (Name, Aff_name) not in current_names:\n",
        "    event_properties = {\n",
        "        'Name': Name,\n",
        "        'Contact_info': '',#str(ClinicalTrial['i.OfficialAffiliation'][i]),\n",
        "        'Aff_name':Aff_name,\n",
        "        'Aff_Zip': ''#str(ClinicalTrial['LocationZip'][i]),\n",
        "    }\n",
        "    create_Researcher_node(event_properties)\n",
        "    city_  = str(ClinicalTrial['LocationCity']).lower().strip()\n",
        "    State_ = str(ClinicalTrial['LocationState']).lower().strip()\n",
        "    event_properties = {\n",
        "        'Aff_country': str(ClinicalTrial['LocationCountry']),\n",
        "        'Aff_state': State_,\n",
        "        'Aff_county': find_fip(city_, State_ )[1] ,\n",
        "        'Aff_city': city_,\n",
        "        'Aff_FIPS': find_fip(city_, State_ )[0]\n",
        "    }\n",
        "    create_Location_node(event_properties, Name, Aff_name)\n",
        "    current_names[(Name, Aff_name)] =1\n",
        "\n",
        "current_names={}\n",
        "extract_and_create_researcher_location_nodes_CT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d603d0c-d62a-4f3f-b579-e5985cbc744f",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "0d603d0c-d62a-4f3f-b579-e5985cbc744f"
      },
      "outputs": [],
      "source": [
        "def extract_and_create_researcher_location_nodes_CT():\n",
        " with driver.session() as session:\n",
        "  query = \"MATCH (i:ClinicalTrial) RETURN i.OfficialName AS OfficialName, i.OfficialAffiliation AS OfficialAffiliation, i.LocationCountry as LocationCountry, i.LocationCity as LocationCity,i.LocationState as LocationState\"\n",
        "  results = session.run(query)\n",
        "  for ClinicalTrial in results:\n",
        "   Name=  ClinicalTrial['OfficialName']\n",
        "   Aff_name=   str(ClinicalTrial['OfficialAffiliation'])\n",
        "   if  classify_entity(Name)==True and (Name, Aff_name) not in current_names:\n",
        "    event_properties = {\n",
        "        'Name': Name,\n",
        "        'Contact_info': '',#str(ClinicalTrial['i.OfficialAffiliation'][i]),\n",
        "        'Aff_name':Aff_name,\n",
        "        'Aff_Zip': ''#str(ClinicalTrial['LocationZip'][i]),\n",
        "    }\n",
        "    create_Researcher_node(event_properties)\n",
        "    city_  = str(ClinicalTrial['LocationCity']).lower().strip()\n",
        "    State_ = str(ClinicalTrial['LocationState']).lower().strip()\n",
        "    event_properties = {\n",
        "        'Aff_country': str(ClinicalTrial['LocationCountry']),\n",
        "        'Aff_state': State_,\n",
        "        'Aff_county': find_fip(city_, State_ )[1] ,\n",
        "        'Aff_city': city_,\n",
        "        'Aff_FIPS': find_fip(city_, State_ )[0]\n",
        "    }\n",
        "    create_Location_node(event_properties, Name, Aff_name)\n",
        "    current_names[(Name, Aff_name)] =1\n",
        "\n",
        "current_names={}\n",
        "extract_and_create_researcher_location_nodes_CT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0e4380-0f5a-4063-a39d-e236520b3454",
      "metadata": {
        "scrolled": true,
        "id": "7d0e4380-0f5a-4063-a39d-e236520b3454",
        "outputId": "84a13594-308b-474b-93ff-2c96913c9c7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: locationtagger in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (0.0.1)\n",
            "Requirement already satisfied: nltk in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from locationtagger) (3.8.1)\n",
            "Requirement already satisfied: spacy in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from locationtagger) (3.7.2)\n",
            "Requirement already satisfied: newspaper3k in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from locationtagger) (0.2.8)\n",
            "Requirement already satisfied: pycountry in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from locationtagger) (24.6.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (4.12.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (10.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (6.0.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (5.3.0)\n",
            "Requirement already satisfied: requests>=2.10.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (2.31.0)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (5.1.2)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k->locationtagger) (0.3)\n",
            "Requirement already satisfied: click in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from nltk->locationtagger) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from nltk->locationtagger) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from nltk->locationtagger) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from nltk->locationtagger) (4.66.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (8.2.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (2.5.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (69.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy->locationtagger) (1.26.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k->locationtagger) (2.5)\n",
            "Requirement already satisfied: six in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from feedfinder2>=0.0.4->newspaper3k->locationtagger) (1.16.0)\n",
            "Requirement already satisfied: sgmllib3k in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from feedparser>=5.2.1->newspaper3k->locationtagger) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->locationtagger) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->locationtagger) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->locationtagger) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.10.0->newspaper3k->locationtagger) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.10.0->newspaper3k->locationtagger) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.10.0->newspaper3k->locationtagger) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.10.0->newspaper3k->locationtagger) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy->locationtagger) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy->locationtagger) (0.1.4)\n",
            "Requirement already satisfied: requests-file>=1.4 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from tldextract>=2.0.1->newspaper3k->locationtagger) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from tldextract>=2.0.1->newspaper3k->locationtagger) (3.12.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->nltk->locationtagger) (0.4.6)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.4.0,>=0.1.0->spacy->locationtagger) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy->locationtagger) (2.1.3)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: spacy in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (8.2.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (69.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.26.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\valinejadj2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\valinejadj2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to\n",
            "[nltk_data]     C:\\Users\\valinejadj2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]     C:\\Users\\valinejadj2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\valinejadj2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\valinejadj2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     --- ------------------------------------ 1.0/12.8 MB 7.1 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 2.4/12.8 MB 7.4 MB/s eta 0:00:02\n",
            "     --------- ------------------------------ 3.1/12.8 MB 5.9 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 4.2/12.8 MB 5.7 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 5.5/12.8 MB 5.8 MB/s eta 0:00:02\n",
            "     ------------------- -------------------- 6.3/12.8 MB 5.4 MB/s eta 0:00:02\n",
            "     ---------------------- ----------------- 7.1/12.8 MB 5.1 MB/s eta 0:00:02\n",
            "     ------------------------ --------------- 7.9/12.8 MB 5.0 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.9/12.8 MB 5.0 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 10.0/12.8 MB 5.1 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 10.7/12.8 MB 4.9 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 11.3/12.8 MB 4.7 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 11.8/12.8 MB 4.5 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 12.1/12.8 MB 4.5 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 12.3/12.8 MB 4.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 3.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
            "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: lxml[html_clean] in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (5.3.0)\n",
            "Requirement already satisfied: lxml-html-clean in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from lxml[html_clean]) (0.3.1)\n"
          ]
        }
      ],
      "source": [
        "#!pip install torch torchvision torchaudio\n",
        "!pip install locationtagger\n",
        "!pip install spacy\n",
        "import nltk\n",
        "import spacy\n",
        "# essential entity models downloads\n",
        "nltk.downloader.download('maxent_ne_chunker')\n",
        "nltk.downloader.download('words')\n",
        "nltk.downloader.download('treebank')\n",
        "nltk.downloader.download('maxent_treebank_pos_tagger')\n",
        "nltk.downloader.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install lxml[html_clean]\n",
        "import locationtagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a897d986-7f36-440c-ae80-3e62f1aae4b3",
      "metadata": {
        "id": "a897d986-7f36-440c-ae80-3e62f1aae4b3"
      },
      "outputs": [],
      "source": [
        "states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\",\n",
        "    \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\",\n",
        "    \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\",\n",
        "    \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n",
        "    \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\",\n",
        "    \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "states_lower = [state.lower() for state in states]\n",
        "states_lower_set = set(state.lower() for state in states)\n",
        "\n",
        "def location_information(j):\n",
        " if j:\n",
        "    place_entity = locationtagger.find_locations(text=j)\n",
        "    if place_entity.countries and place_entity.countries[0] != 'United States':\n",
        "       if place_entity.countries:country = place_entity.countries[0]\n",
        "       else:country = None  # or some default value\n",
        "       if place_entity.regions:region = place_entity.regions[0]\n",
        "       else: region = None  # or some default value\n",
        "       if place_entity.cities: city = place_entity.cities[0]\n",
        "       else:city = None  # or some default value\n",
        "       return country, region, city\n",
        "    else:\n",
        "     if place_entity.countries or (place_entity.regions and place_entity.regions[0].lower() in states_lower_set):\n",
        "        # If country is specified as US or region matches a state, proceed\n",
        "        city_lower=None\n",
        "        for i in reversed(place_entity.cities):\n",
        "            city_lower = i.lower()\n",
        "            if city_lower in fips_finding_dict and city_lower not in ['hospital', 'university']:\n",
        "                   break\n",
        "        if place_entity.countries:country = place_entity.countries[0]\n",
        "        else:country = None  # or some default value\n",
        "        if place_entity.regions:region = place_entity.regions[0]\n",
        "        else: region = None  # or some default value\n",
        "        if city_lower: city = city_lower\n",
        "        else:city = None  # or some default value\n",
        "        return country, region, city\n",
        "    return None, None, None\n",
        " return None, None, None\n",
        "\n",
        "def extract_and_create_researcher_location_nodes_PubMed():\n",
        " with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "\n",
        "  with driver.session() as session:\n",
        "   query = \"MATCH (i:Pubmed) RETURN i.fullName AS fullName, i.ar_affiliation AS affiliation\"\n",
        "   results = session.run(query)\n",
        "   for Pubmed in results:\n",
        "    A,B=Pubmed['fullName'], str(Pubmed['affiliation']).replace('[','').replace(']','')\n",
        "    if  (A, B) not in current_names:\n",
        "     if B in pubmed_institution:\n",
        "        place_entity=pubmed_institution[B]\n",
        "     else:\n",
        "       place_entity =location_information(B)\n",
        "       pubmed_institution[B]= place_entity\n",
        "\n",
        "     event_properties = {\n",
        "        'Name': str(A),\n",
        "        'Contact_info': '',#str(Pubmed['i.OfficialAffiliation'][i]),\n",
        "        'Aff_name': B,\n",
        "        'Aff_Zip': ''  }\n",
        "     create_Researcher_node(event_properties)\n",
        "     current_names[(A, B)] =1\n",
        "     if place_entity[0] != None and place_entity[1] !=  None and place_entity[2] !=  None:\n",
        "      event_properties = {\n",
        "         'Aff_country': place_entity[0],\n",
        "        'Aff_state':   place_entity[1],\n",
        "        'Aff_county': find_fip(place_entity[2], place_entity[1] )[1],\n",
        "        'Aff_city': place_entity[2],\n",
        "        'Aff_FIPS': find_fip(place_entity[2], place_entity[1] )[0]  }\n",
        "      create_Location_node(event_properties, A, B)\n",
        "\n",
        "pubmed_institution={}\n",
        "current_names={}\n",
        "extract_and_create_researcher_location_nodes_PubMed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19dc36f2-0568-4632-acb2-3acc21847578",
      "metadata": {
        "id": "19dc36f2-0568-4632-acb2-3acc21847578"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "organ_address=pd.read_csv('C:/Users/Desktop/Grant_org1.csv')\n",
        "organ_address['n.org_name']=organ_address.apply(lambda x: x['n.org_name'].replace('\"','').lower().strip(),axis=1)\n",
        "\n",
        "def extract_and_create_researcher_location_nodes_Grant():\n",
        " with driver.session() as session:\n",
        "  query = \"MATCH (i:Grant) RETURN i.org_name AS org_name, i.pi_name AS pi_name\"\n",
        "  results = session.run(query)\n",
        "  for Grant in results:\n",
        "   Name= Grant['pi_name']\n",
        "   Aff_name= str(Grant['org_name'])\n",
        "   if (Name, Aff_name) not in current_names:\n",
        "     data=organ_address[organ_address['n.org_name'] == Grant['org_name'].lower().strip()]\n",
        "     '''\n",
        "      or\n",
        "      from geopy.geocoders import Nominatim\n",
        "      geolocator = Nominatim(user_agent=\"GetLoc\", timeout=30)  # You can adjust the timeout value as needed\n",
        "      location = geolocator.geocode('NIH')\n",
        "      if location:\n",
        "              address = location.address.split(\", \")\n",
        "              data={}\n",
        "              data['Country'] = address[-1]\n",
        "              data['zip_code'] = address[-2]\n",
        "              data['state']  = address[-3]  # Massachusetts (state)\n",
        "              data['County'] = address[-4]  # Massachusetts (state)\n",
        "              data['city'] = address[-5]  # Boston (city)\n",
        "     '''\n",
        "     if len(data) >0 :\n",
        "       event_properties = {\n",
        "        'Name': Name,\n",
        "        'Contact_info': '',\n",
        "        'Aff_name': Aff_name,\n",
        "        'Aff_Zip':  data.iloc[0]['zip_code']\n",
        "       }\n",
        "       create_Researcher_node(event_properties)\n",
        "       A,B,C=data.iloc[0]['Country'], data.iloc[0]['state'], data.iloc[0]['County']\n",
        "       if  type(A)==str and type(B)==str and type(C) == str:\n",
        "        event_properties = {\n",
        "        'Aff_country': A ,\n",
        "        'Aff_state': B,\n",
        "        'Aff_county': C,\n",
        "        'Aff_city': data.iloc[0]['city'],\n",
        "        'Aff_FIPS': find_fip(data.iloc[0]['city'], data.iloc[0]['state'] )[0]\n",
        "        }\n",
        "        create_Location_node(event_properties, Name, Aff_name)\n",
        "     else:\n",
        "         event_properties = {'Name': Name,'Contact_info': '', 'Aff_name': Aff_name, 'Aff_Zip': ''    }\n",
        "         create_Researcher_node(event_properties)\n",
        "     current_names[(Name, Aff_name)] =1\n",
        "\n",
        "current_names={}\n",
        "extract_and_create_researcher_location_nodes_Grant()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb12a15b-2645-453d-acf1-b342788f66ca",
      "metadata": {
        "id": "fb12a15b-2645-453d-acf1-b342788f66ca"
      },
      "outputs": [],
      "source": [
        "def create_relationship_between_nodes_1(node1_label, node2_label, relationship_type):\n",
        "    with driver.session() as session:\n",
        "        query = (\n",
        "            f\"MATCH (n1:{node1_label}), (n2:{node2_label}) \"\n",
        "            f\"WHERE n1.Name = n2.OfficialName AND n1.Aff_name = n2.OfficialAffiliation \"\n",
        "            f\"CREATE (n1)-[:{relationship_type}]->(n2)\"\n",
        "        )\n",
        "        session.run(query)\n",
        "def create_relationship_between_nodes_2(node1_label, node2_label, relationship_type):\n",
        "    with driver.session() as session:\n",
        "        query = (\n",
        "        f\"MATCH (n1:{node1_label}), (n2:{node2_label}) \"\n",
        "        f\"WHERE n1.Name = n2.pi_name and  n1.Aff_name= n2.org_name \"\n",
        "       f\"CREATE (n1)-[:{relationship_type}]->(n2)\"\n",
        "        )\n",
        "        session.run(query)\n",
        "def create_relationship_between_nodes_3(node1_label, node2_label, relationship_type):\n",
        "  with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "    with driver.session() as session:\n",
        "        query = (\n",
        "        f\"MATCH (n1:{node1_label}), (n2:{node2_label}) \"\n",
        "        f\"WHERE n1.Name = n2.fullName and  n1.Aff_name= n2.ar_affiliation \"\n",
        "       f\"CREATE (n1)-[:{relationship_type}]->(n2)\"\n",
        "        )\n",
        "        session.run(query)\n",
        "\n",
        "create_relationship_between_nodes_1('Researcher','ClinicalTrial',  \"Involves_Researcher_ct\")\n",
        "create_relationship_between_nodes_2('Researcher','Grant',  \"Involves_Researcher_g\")\n",
        "\n",
        "create_relationship_between_nodes_3('Researcher','Pubmed',  \"Involves_Researcher_p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f64ff6-8943-41ae-9a4b-0eb2c893a0bb",
      "metadata": {
        "id": "17f64ff6-8943-41ae-9a4b-0eb2c893a0bb",
        "outputId": "d3c0cb58-62af-4770-ab54-2cdb97f03caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- start ---\n",
            "--- 0.78605055809021 seconds ---\n",
            "--- 5.310334920883179 seconds ---\n",
            "--- 119.05593013763428 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#faster way\n",
        "def find_coauthors(df, name, affiliation, title):\n",
        "    coauthors_dict = {}\n",
        "    connection = {}\n",
        "    # Group rows by title\n",
        "    grouped = df.groupby(title)\n",
        "    for title_value, group in grouped:\n",
        "        author_affiliation_set = set()\n",
        "\n",
        "        for _, row in group.iterrows():\n",
        "            author = (row[name], row[affiliation])\n",
        "\n",
        "            # Skip if author name or affiliation is empty\n",
        "            if not all(author):\n",
        "                continue\n",
        "\n",
        "            # If the author is not already seen for this title\n",
        "            if author not in author_affiliation_set:\n",
        "                author_affiliation_set.add(author)\n",
        "            else:\n",
        "                # This author has already been considered for this title\n",
        "                continue\n",
        "\n",
        "            # Add author to coauthors_dict if not already present\n",
        "            if author not in coauthors_dict:\n",
        "                coauthors_dict[author] = []\n",
        "\n",
        "            # Find co-authors within the same title group\n",
        "            coauthors = group[(group[name] != row[name]) | (group[affiliation] != row[affiliation])]\n",
        "            for _, coauthor_row in coauthors.iterrows():\n",
        "                coauthor = (coauthor_row[name], coauthor_row[affiliation])\n",
        "\n",
        "                # Skip if coauthor name or affiliation is empty\n",
        "                if not all(coauthor):\n",
        "                    continue\n",
        "\n",
        "                # Avoid duplicates\n",
        "                if coauthor not in coauthors_dict[author]:\n",
        "                    coauthors_dict[author].append(coauthor)\n",
        "\n",
        "                # Update connection count\n",
        "                connection[(author, coauthor)] = connection.get((author, coauthor), 0) + 1\n",
        "\n",
        "    return coauthors_dict, connection\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def merge_dicts(d1, d2):\n",
        "    combined_dict = defaultdict(set)\n",
        "    for key, values in d1.items():\n",
        "        combined_dict[key].update(values)\n",
        "    for key, values in d2.items():\n",
        "        combined_dict[key].update(values)\n",
        "    return combined_dict\n",
        "\n",
        "\n",
        "\n",
        "def sum_dicts(dict1, dict2):\n",
        "    result_dict = {}\n",
        "    all_keys = set(dict1.keys()) | set(dict2.keys())\n",
        "    for key in all_keys:\n",
        "        value1 = dict1.get(key, 0)\n",
        "        value2 = dict2.get(key, 0)\n",
        "        result_dict[key] = value1 + value2\n",
        "    return result_dict\n",
        "def extract_and_create_researcher_connection():\n",
        " with driver.session() as session:\n",
        "  query = \"MATCH (i:ClinicalTrial) RETURN i.OfficialName AS OfficialName, i.OfficialAffiliation AS OfficialAffiliation, i.NCTId as NCTId \"\n",
        "  results = session.run(query)\n",
        "  data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "  ClinicalTrial = pd.DataFrame(data)\n",
        "  print(\"--- start ---\")\n",
        "  start_time = time.time()\n",
        "  a,a1=find_coauthors(ClinicalTrial,'OfficialName','OfficialAffiliation','NCTId')\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  ######   Grant\n",
        "  query = \"MATCH (i:Grant) RETURN i.pi_name AS pi_name, i.org_name AS org_name, i.application_id as application_id \"\n",
        "  results = session.run(query)\n",
        "  data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "  Grant = pd.DataFrame(data)\n",
        "  start_time = time.time()\n",
        "  b,b1=find_coauthors(Grant,'pi_name','org_name','application_id')\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  ##################### pubmed\n",
        "  query = \"MATCH (i:Pubmed) RETURN i.fullName AS fullName, i.ar_affiliation AS affiliation, i.pubmed_id as pubmed_id \"\n",
        "  results = session.run(query)\n",
        "  data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "  Pubmed = pd.DataFrame(data)\n",
        "  start_time = time.time()\n",
        "  c,c1=find_coauthors(Pubmed,'fullName','affiliation','pubmed_id')\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  ###################\n",
        "  combined_dict = merge_dicts(a, b)\n",
        "  combined_dict = merge_dicts(combined_dict, c)\n",
        "  ###########\n",
        "  combined_dict1 = sum_dicts(a1, b1)\n",
        "  result_dict1 = sum_dicts(combined_dict1, c1)\n",
        "  for key in result_dict1:\n",
        "    result_dict1[key] = result_dict1[key] %1000\n",
        "  ###########\n",
        "  result_dict = {key: list(values) for key, values in combined_dict.items()}\n",
        "  return combined_dict, result_dict1, a,a1,b,b1,c,c1\n",
        "result_dict, result_dict1, a,a1,b,b1,c,c1=extract_and_create_researcher_connection()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7292179f-284f-44a1-a12b-990e7cc24740",
      "metadata": {
        "id": "7292179f-284f-44a1-a12b-990e7cc24740",
        "outputId": "a6a3e09d-24b2-457b-f2c5-c05beaf93b7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "778210\n"
          ]
        }
      ],
      "source": [
        "n=0\n",
        "for i in result_dict1:\n",
        "    n+=1\n",
        "print(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15073bad-a7ab-421d-acdb-49ff6c77baa1",
      "metadata": {
        "id": "15073bad-a7ab-421d-acdb-49ff6c77baa1"
      },
      "outputs": [],
      "source": [
        "gg=Grant.groupby('application_id')\n",
        "bb=pd.DataFrame(gg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b24077d-1811-410a-8816-5282dbb33a7f",
      "metadata": {
        "id": "6b24077d-1811-410a-8816-5282dbb33a7f",
        "outputId": "e5cd8776-37d7-485c-f3b8-9b39cfb376f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "542"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pi_name</th>\n",
              "      <th>org_name</th>\n",
              "      <th>application_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13317</th>\n",
              "      <td>PARTIDA-SANCHEZ, SANTIAGO</td>\n",
              "      <td>RESEARCH INST NATIONWIDE CHILDREN'S HOSP</td>\n",
              "      <td>10445615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13318</th>\n",
              "      <td>AMER, AMAL O</td>\n",
              "      <td>RESEARCH INST NATIONWIDE CHILDREN'S HOSP</td>\n",
              "      <td>10445615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13319</th>\n",
              "      <td>HALL-STOODLEY, LUANNE</td>\n",
              "      <td>RESEARCH INST NATIONWIDE CHILDREN'S HOSP</td>\n",
              "      <td>10445615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13320</th>\n",
              "      <td>KOPP, BENJAMIN T.</td>\n",
              "      <td>RESEARCH INST NATIONWIDE CHILDREN'S HOSP</td>\n",
              "      <td>10445615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         pi_name                                  org_name  \\\n",
              "13317  PARTIDA-SANCHEZ, SANTIAGO  RESEARCH INST NATIONWIDE CHILDREN'S HOSP   \n",
              "13318               AMER, AMAL O  RESEARCH INST NATIONWIDE CHILDREN'S HOSP   \n",
              "13319      HALL-STOODLEY, LUANNE  RESEARCH INST NATIONWIDE CHILDREN'S HOSP   \n",
              "13320          KOPP, BENJAMIN T.  RESEARCH INST NATIONWIDE CHILDREN'S HOSP   \n",
              "\n",
              "      application_id  \n",
              "13317       10445615  \n",
              "13318       10445615  \n",
              "13319       10445615  \n",
              "13320       10445615  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for i in range(10000):\n",
        "    if bb[1][i].shape[0] >3:\n",
        "         display(i,bb[1][i])\n",
        "         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf1c9e6a-39df-4548-b52c-7eddbf35664d",
      "metadata": {
        "id": "bf1c9e6a-39df-4548-b52c-7eddbf35664d"
      },
      "outputs": [],
      "source": [
        "for i in coauthors_dict:\n",
        "    if len(coauthors_dict[i])>0: print(coauthors_dict[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e68144-1e05-42b7-8e30-524a6c4e0e7c",
      "metadata": {
        "id": "f9e68144-1e05-42b7-8e30-524a6c4e0e7c"
      },
      "outputs": [],
      "source": [
        "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "\n",
        "def remove_collaborated_with_edges():\n",
        "    with driver.session() as session:\n",
        "        session.run(\"MATCH ()-[r:Collaborated_With]->() DELETE r\")\n",
        "\n",
        "remove_collaborated_with_edges()\n",
        "driver.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84589a3-c8e8-4119-a19d-06d2e7fe14aa",
      "metadata": {
        "id": "d84589a3-c8e8-4119-a19d-06d2e7fe14aa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import threading\n",
        "\n",
        "# Function to prevent sleep\n",
        "def prevent_sleep():\n",
        "    while True:\n",
        "        time.sleep(60)  # Keep running indefinitely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a7ad71c-d8cb-4d45-b28c-b4dba852e289",
      "metadata": {
        "id": "3a7ad71c-d8cb-4d45-b28c-b4dba852e289"
      },
      "outputs": [],
      "source": [
        "def create_relationship_between_Researchers(n1_Name, n1_Aff_name, n2_Name, n2_Aff_name, num_colab):\n",
        "  with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "    with driver.session() as session:\n",
        "        # Ensure no duplicate relationships are created\n",
        "        create_query = (\n",
        "            \"MATCH (n1:Researcher {Name: $n1_Name, Aff_name: $n1_Aff_name}), \"\n",
        "            \"(n2:Researcher {Name: $n2_Name, Aff_name: $n2_Aff_name}) \"\n",
        "            \"MERGE (n1)-[r:Collaborated_With]-(n2) \"\n",
        "            \"ON CREATE SET r.num_colab = $num_colab \"\n",
        "            \"ON MATCH SET r.num_colab = r.num_colab + $num_colab\"\n",
        "        )\n",
        "        session.run(create_query, n1_Name=n1_Name, n1_Aff_name=n1_Aff_name, n2_Name=n2_Name, n2_Aff_name=n2_Aff_name, num_colab=num_colab)\n",
        "#61050--- 130,000\n",
        "#204485----290485\n",
        "n=0\n",
        "for key in result_dict1:\n",
        "   if n> 277633 and n <290485:\n",
        "    create_relationship_between_Researchers(key[0][0], key[0][1], key[1][0], key[1][1], result_dict1[key])\n",
        "   n+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460053d8-1720-4d2a-a9c1-20736676f442",
      "metadata": {
        "id": "460053d8-1720-4d2a-a9c1-20736676f442",
        "outputId": "b1ff5926-e47a-42d4-dba3-b9cbcd11eefa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "778210\n"
          ]
        }
      ],
      "source": [
        "n=0\n",
        "for key in result_dict1:\n",
        "    n+=1\n",
        "print(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "113b69da-7ca9-4221-87e3-9440b35e9dd1",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "113b69da-7ca9-4221-87e3-9440b35e9dd1"
      },
      "source": [
        "# US MAp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf216bf7-5a3c-4f2b-a709-a60b90439c75",
      "metadata": {
        "id": "cf216bf7-5a3c-4f2b-a709-a60b90439c75",
        "outputId": "a01082e4-1953-4d7c-9b67-730fff51b6db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\valinejadj2\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting python-louvain\n",
            "  Using cached python-louvain-0.16.tar.gz (204 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting networkx (from python-louvain)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting numpy (from python-louvain)\n",
            "  Downloading numpy-2.2.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
            "   ------------------------ --------------- 1.0/1.7 MB 5.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.7/1.7 MB 5.5 MB/s eta 0:00:00\n",
            "Downloading numpy-2.2.1-cp313-cp313-win_amd64.whl (12.6 MB)\n",
            "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
            "   ------ --------------------------------- 2.1/12.6 MB 10.3 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 5.0/12.6 MB 12.6 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 8.7/12.6 MB 14.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.6/12.6 MB 16.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.6/12.6 MB 13.8 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: python-louvain\n",
            "  Building wheel for python-louvain (pyproject.toml): started\n",
            "  Building wheel for python-louvain (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for python-louvain: filename=python_louvain-0.16-py3-none-any.whl size=9448 sha256=ddb093d8f814a2904a0aa3eea64d7ce1ecfbe225f3fadbf590634cd6e29266d4\n",
            "  Stored in directory: c:\\users\\valinejadj2\\appdata\\local\\pip\\cache\\wheels\\ee\\52\\54\\7ecd0f1ebf5f5a8466f70a27ed2b94d20b955376879d6159c5\n",
            "Successfully built python-louvain\n",
            "Installing collected packages: numpy, networkx, python-louvain\n",
            "Successfully installed networkx-3.4.2 numpy-2.2.1 python-louvain-0.16\n"
          ]
        }
      ],
      "source": [
        "!pip install python-louvain\n",
        "import networkx as nx\n",
        "layout_algorithms = [\n",
        "    nx.circular_layout,\n",
        "    nx.random_layout,\n",
        "    nx.spring_layout,\n",
        "    nx.spectral_layout,\n",
        "    nx.shell_layout,\n",
        "    nx.bipartite_layout,\n",
        "    nx.planar_layout,\n",
        "    nx.fruchterman_reingold_layout,\n",
        "    nx.kamada_kawai_layout\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8648cd-3738-4304-b293-b8ae04e5ba4f",
      "metadata": {
        "id": "0b8648cd-3738-4304-b293-b8ae04e5ba4f",
        "outputId": "49053552-280c-461f-a813-f45993f26e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- start ---\n",
            "--- 0.3788144588470459 seconds ---\n",
            "--- 4.489328145980835 seconds ---\n",
            "--- 2.0419180393218994 seconds ---\n",
            "3455\n"
          ]
        }
      ],
      "source": [
        "#faster way\n",
        "import pandas as pd\n",
        "def find_coauthors(df, name, affiliation, title):\n",
        "    coauthors_dict = {}\n",
        "    connection = {}\n",
        "    # Group rows by title\n",
        "    grouped = df.groupby(title)\n",
        "    for title_value, group in grouped:\n",
        "        author_affiliation_set = set()\n",
        "\n",
        "        for _, row in group.iterrows():\n",
        "            author = (row[name], row[affiliation])\n",
        "\n",
        "            # Skip if author name or affiliation is empty\n",
        "            if not all(author):\n",
        "                continue\n",
        "\n",
        "            # If the author is not already seen for this title\n",
        "            if author not in author_affiliation_set:\n",
        "                author_affiliation_set.add(author)\n",
        "            else:\n",
        "                # This author has already been considered for this title\n",
        "                continue\n",
        "\n",
        "            # Add author to coauthors_dict if not already present\n",
        "            if author not in coauthors_dict:\n",
        "                coauthors_dict[author] = []\n",
        "\n",
        "            # Find co-authors within the same title group\n",
        "            coauthors = group[(group[name] != row[name]) | (group[affiliation] != row[affiliation])]\n",
        "            for _, coauthor_row in coauthors.iterrows():\n",
        "                coauthor = (coauthor_row[name], coauthor_row[affiliation])\n",
        "\n",
        "                # Skip if coauthor name or affiliation is empty\n",
        "                if not all(coauthor):\n",
        "                    continue\n",
        "\n",
        "                # Avoid duplicates\n",
        "                if coauthor not in coauthors_dict[author]:\n",
        "                    coauthors_dict[author].append(coauthor)\n",
        "\n",
        "                # Update connection count\n",
        "                connection[(author, coauthor)] = connection.get((author, coauthor), 0) + 1\n",
        "\n",
        "    return coauthors_dict, connection\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def merge_dicts(d1, d2):\n",
        "    combined_dict = defaultdict(set)\n",
        "    for key, values in d1.items():\n",
        "        combined_dict[key].update(values)\n",
        "    for key, values in d2.items():\n",
        "        combined_dict[key].update(values)\n",
        "    return combined_dict\n",
        "\n",
        "\n",
        "\n",
        "def sum_dicts(dict1, dict2):\n",
        "    result_dict = {}\n",
        "    all_keys = set(dict1.keys()) | set(dict2.keys())\n",
        "    for key in all_keys:\n",
        "        value1 = dict1.get(key, 0)\n",
        "        value2 = dict2.get(key, 0)\n",
        "        result_dict[key] = value1 + value2\n",
        "    return result_dict\n",
        "def extract_and_create_researcher_connection(disease):\n",
        " with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "  with driver.session() as session:\n",
        "   query = \"MATCH (l:Location)--(r:Researcher)--(i:ClinicalTrial)--(g:Gard) where g.GARDname= $disease and COALESCE(size(l.Aff_FIPS), 0) > 3    RETURN i.OfficialName AS OfficialName, i.OfficialAffiliation AS OfficialAffiliation, i.NCTId as NCTId, l.Aff_FIPS as FIPS \"\n",
        "   results = session.run(query, disease=disease)\n",
        "   data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "   ClinicalTrial = pd.DataFrame(data)\n",
        "   print(\"--- start ---\")\n",
        "   start_time = time.time()\n",
        "   if ClinicalTrial.shape[0]>0: a,a1=find_coauthors(ClinicalTrial,'OfficialName','OfficialAffiliation','NCTId')\n",
        "   else: a,a1=pd.DataFrame(),pd.DataFrame()\n",
        "   print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "   ######   Grant\n",
        "   query = \"MATCH (l:Location)--(r:Researcher)--(i:Grant)--(g:Gard) where g.GARDname= $disease and COALESCE(size(l.Aff_FIPS), 0) > 3    RETURN i.pi_name AS pi_name, i.org_name AS org_name, i.application_id as application_id , l.Aff_FIPS as FIPS \"\n",
        "   results = session.run(query, disease=disease)\n",
        "   data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "   Grant = pd.DataFrame(data)\n",
        "   start_time = time.time()\n",
        "   if Grant.shape[0]>0:b,b1=find_coauthors(Grant,'pi_name','org_name','application_id')\n",
        "   else: b,b1=pd.DataFrame(),pd.DataFrame()\n",
        "   print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "   ##################### pubmed\n",
        "   query = \"MATCH (l:Location)--(r:Researcher)--(i:Pubmed)--(g:Gard) where g.GARDname= $disease and COALESCE(size(l.Aff_FIPS), 0) > 3   RETURN i.fullName AS fullName, i.ar_affiliation AS affiliation, i.pubmed_id as pubmed_id, l.Aff_FIPS as FIPS \"\n",
        "   results = session.run(query, disease=disease)\n",
        "   data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "   Pubmed = pd.DataFrame(data)\n",
        "   start_time = time.time()\n",
        "   if Pubmed.shape[0]>0:c,c1=find_coauthors(Pubmed,'fullName','affiliation','pubmed_id')\n",
        "   else: c,c1=pd.DataFrame(),pd.DataFrame()\n",
        "   print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "   ###################\n",
        "   combined_dict = merge_dicts(a, b)\n",
        "   combined_dict = merge_dicts(combined_dict, c)\n",
        "   ###########\n",
        "   combined_dict1 = sum_dicts(a1, b1)\n",
        "   result_dict1 = sum_dicts(combined_dict1, c1)\n",
        "   for key in result_dict1:\n",
        "    result_dict1[key] = result_dict1[key] %1000\n",
        "   ###########\n",
        "   result_dict = {key: list(values) for key, values in combined_dict.items()}\n",
        "   return combined_dict, result_dict1, a,a1,b,b1,c,c1\n",
        "disease= 'Cystic fibrosis'\n",
        "result_dict, result_dict1, a,a1,b,b1,c,c1=extract_and_create_researcher_connection(disease)\n",
        "print(len(result_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458b0910-560b-41f9-8032-88e7981ec3c6",
      "metadata": {
        "id": "458b0910-560b-41f9-8032-88e7981ec3c6"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import community\n",
        "from community import community_louvain\n",
        "import matplotlib.pyplot as plt\n",
        "# Create a directed graph\n",
        "G = nx.Graph()\n",
        "# Add nodes and edges from the dictionary\n",
        "for nodes, edges in result_dict.items():\n",
        "    G.add_node(nodes)\n",
        "    for edge in edges:\n",
        "        G.add_node(edge)\n",
        "        G.add_edge(nodes, edge)\n",
        "\n",
        "# Detect communities using Louvain method\n",
        "partition = community_louvain.best_partition(G)\n",
        "data_table = pd.DataFrame(partition.items(), columns=['name', 'Community'])\n",
        "data_table['network']=data_table.apply(lambda x: result_dict[x['name']] ,axis=1)\n",
        "index_community_detection= list(set(data_table[data_table['network']!=set()]['Community'].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9449733e-cade-4533-a703-ccc35e023ce8",
      "metadata": {
        "id": "9449733e-cade-4533-a703-ccc35e023ce8"
      },
      "outputs": [],
      "source": [
        "def merge_community_detection_and_researchers_info(disease):\n",
        " with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "  with driver.session() as session:\n",
        "   query = \"MATCH (l:Location)--(r:Researcher)--(i:ClinicalTrial)--(g:Gard) WHERE g.GARDname= $disease and COALESCE(size(l.Aff_FIPS), 0) > 3 RETURN i.OfficialName as OfficialName , i.OfficialAffiliation as OfficialAffiliation, i.OfficialTitle as OfficialTitle  ,i.Interventions as concatenatedInterventions, i.BriefSummary as BriefSummary, l.Aff_FIPS as FIPS\"\n",
        "   results = session.run(query, disease=disease)\n",
        "   data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "   if data==[]:  ClinicalTrial = pd.DataFrame(columns=[\"OfficialName\", \"OfficialAffiliation\", \"OfficialTitle\", \"concatenatedInterventions\", \"BriefSummary,FIPS\"])\n",
        "   else: ClinicalTrial = pd.DataFrame(data)\n",
        "   ######   Grant\n",
        "   query = \"MATCH (l:Location)--(r:Researcher)--(i:Grant)--(g:Gard) WHERE g.GARDname= $disease and COALESCE(size(l.Aff_FIPS), 0) > 3 RETURN i.pi_name as pi_name,i.org_name as org_name,i.title as title ,i.terms as terms,i.abstract as abstract, l.Aff_FIPS as FIPS\"\n",
        "   results = session.run(query, disease=disease)\n",
        "   data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "\n",
        "   if data==[]:  Grant = pd.DataFrame(columns=[\"pi_name\", \"org_name\", \"title\", \"terms\", \"abstract\",\"FIPS\"])\n",
        "   else: Grant = pd.DataFrame(data)\n",
        "   ##################### pubmed\n",
        "   query = \"MATCH (l:Location)--(r:Researcher)--(i:Pubmed)--(g:Gard) WHERE g.GARDname= $disease and COALESCE(size(l.Aff_FIPS), 0) > 3 RETURN i.fullName as fullName, i.ar_affiliation as affiliation, i.title as title, i.concatenatedKeywords as concatenatedKeywords,i.abstractText as abstractText, l.Aff_FIPS as FIPS \"\n",
        "   results = session.run(query, disease=disease)\n",
        "   data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "\n",
        "   if data==[]:  Pubmed = pd.DataFrame(columns=[\"fullName\", \"affiliation\", \"title\", \"concatenatedKeywords\",\"abstractText\", \"FIPS\"])\n",
        "   else: Pubmed = pd.DataFrame(data)\n",
        "   ################\n",
        "   if ClinicalTrial.shape[0]>0:  ClinicalTrial['name'] = ClinicalTrial.apply(lambda row: (row['OfficialName'], row['OfficialAffiliation']), axis=1)\n",
        "   else: ClinicalTrial['name'] = None\n",
        "   if Grant.shape[0]>0:          Grant['name'] = Grant.apply(lambda row: (row['pi_name'], row['org_name']), axis=1)\n",
        "   else: Grant['name'] = None\n",
        "   if Pubmed.shape[0]>0:         Pubmed['name'] = Pubmed.apply(lambda row: (row['fullName'], row['affiliation']), axis=1)\n",
        "   else: Pubmed['name'] = None\n",
        "\n",
        "   ClinicalTrial['BriefSummary']=ClinicalTrial['OfficialTitle'].fillna(\"\").astype(str)+' '+ ClinicalTrial['BriefSummary'].fillna(\"\").astype(str)   #+ClinicalTrial['concatenatedInterventions'].fillna(\"\").astype(str)+\n",
        "   Grant['BriefSummary']=Grant['title'].fillna(\"\").astype(str)+' '+ Grant['abstract'].fillna(\"\").astype(str)                   #+Grant['terms'].fillna(\"\").astype(str)\n",
        "   Pubmed['abstractText']=Pubmed['title'].fillna(\"\").astype(str)+' '+ Pubmed['abstractText'].fillna(\"\").astype(str)    #+Pubmed['concatenatedKeywords'].fillna(\"\").astype(str)\n",
        "\n",
        "   ClinicalTrial['Terms']=ClinicalTrial['concatenatedInterventions']\n",
        "   Grant['Terms']=Grant['terms']\n",
        "   Pubmed['Terms']=Pubmed['concatenatedKeywords']\n",
        "   #df1=ClinicalTrial[['i.OfficialName',  'c.BriefSummary', 'i.OfficialAffiliation','name']]\n",
        "   df1=ClinicalTrial[['OfficialName',  'BriefSummary','OfficialAffiliation', 'name','Terms',\"FIPS\"]]\n",
        "   df2=Grant[['pi_name', 'abstract', 'org_name','name','Terms',\"FIPS\"]]\n",
        "   df3=Pubmed[['fullName',  'abstractText', 'affiliation','name','Terms',\"FIPS\"]]\n",
        "   # Rename columns to avoid conflicts\n",
        "   df1.columns = ['PI','Abstract', 'affiliation','name','Terms',\"FIPS\"]\n",
        "   df2.columns = ['PI','Abstract', 'affiliation','name','Terms',\"FIPS\"]\n",
        "   df3.columns = ['PI','Abstract', 'affiliation','name','Terms',\"FIPS\"]\n",
        "   #df2['Location']=  df2['locationCity'] +' , '+df2['Location']\n",
        "   result_df = pd.concat([df1  , df2, df3])\n",
        "   #result_df = pd.concat([df1,  df3])\n",
        "   result_df=result_df.drop_duplicates()\n",
        "   result_df.index= range(result_df.shape[0])\n",
        "   #################################################### merging with c\n",
        "\n",
        "   merged_data1 = pd.merge(result_df,data_table , on='name', how='outer')\n",
        "   concatenated_data = merged_data1.drop_duplicates(subset=['name'])\n",
        "   concatenated_data.reset_index(drop=True, inplace=True)\n",
        "   #concatenated_data=concatenated_data.drop(['name'],axis=1)\n",
        "   return concatenated_data\n",
        "   '''\n",
        "   #return result_df\n",
        "   '''\n",
        "concatenated_data=merge_community_detection_and_researchers_info(disease)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "923e8937-c213-4bcb-8655-d23b0c6911d8",
      "metadata": {
        "id": "923e8937-c213-4bcb-8655-d23b0c6911d8",
        "outputId": "1435fec3-e15a-4fc8-b18c-6e79022cce0c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PI</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>name</th>\n",
              "      <th>Terms</th>\n",
              "      <th>FIPS</th>\n",
              "      <th>Community</th>\n",
              "      <th>network</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Anthony Bier</td>\n",
              "      <td>A Prospective, Randomized, Double-blinded, Pla...</td>\n",
              "      <td>Nutrasource Pharmaceutical and Nutraceutical S...</td>\n",
              "      <td>(Anthony Bier, Nutrasource Pharmaceutical and ...</td>\n",
              "      <td>[Placebo, Probiotic]</td>\n",
              "      <td>12086</td>\n",
              "      <td>280.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mina Pastagia</td>\n",
              "      <td>A Phase 2, Multi-Center, Double-Blind, Randomi...</td>\n",
              "      <td>Armata Pharmaceuticals, Inc.</td>\n",
              "      <td>(Mina Pastagia, Armata Pharmaceuticals, Inc.)</td>\n",
              "      <td>[Placebo, AP-PA02]</td>\n",
              "      <td>53033</td>\n",
              "      <td>253.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mary C. Drinane</td>\n",
              "      <td>Relationship Between the Development of Impair...</td>\n",
              "      <td>Dartmouth-Hitchcock Medical Center</td>\n",
              "      <td>(Mary C. Drinane, Dartmouth-Hitchcock Medical ...</td>\n",
              "      <td>[Fibroscan]</td>\n",
              "      <td>33009</td>\n",
              "      <td>271.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>H. Frank Farmer</td>\n",
              "      <td>An Open-Label Phase 1 Study to Examine the Eff...</td>\n",
              "      <td>Covance CRU, Inc.</td>\n",
              "      <td>(H. Frank Farmer, Covance CRU, Inc.)</td>\n",
              "      <td>[VX-770]</td>\n",
              "      <td>12127</td>\n",
              "      <td>91.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lisa Guay-Woodford</td>\n",
              "      <td>Core A: The Hepato/Renal Fibrocystic Diseases ...</td>\n",
              "      <td>Childrens National Health System</td>\n",
              "      <td>(Lisa Guay-Woodford, Childrens National Health...</td>\n",
              "      <td>[]</td>\n",
              "      <td>11001</td>\n",
              "      <td>118.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3537</th>\n",
              "      <td>Greenberg J</td>\n",
              "      <td>&lt;h4&gt;Rationale&lt;/h4&gt;Monitoring clinical disease...</td>\n",
              "      <td>Division of Respiratory Diseases, Boston Child...</td>\n",
              "      <td>(Greenberg J, Division of Respiratory Diseases...</td>\n",
              "      <td>[]</td>\n",
              "      <td>25025</td>\n",
              "      <td>1769.0</td>\n",
              "      <td>{(Walt DR, Department of Chemistry, Tufts Univ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3538</th>\n",
              "      <td>Diggle SP</td>\n",
              "      <td>&lt;i&gt;Pseudomonas aeruginosa&lt;/i&gt; is an opportuni...</td>\n",
              "      <td>Center for Microbial Dynamics and Infection, S...</td>\n",
              "      <td>(Diggle SP, Center for Microbial Dynamics and ...</td>\n",
              "      <td>[evolution, antibiotic resistance, population ...</td>\n",
              "      <td>13121</td>\n",
              "      <td>2072.0</td>\n",
              "      <td>{(Vanderwoude J, Center for Microbial Dynamics...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3539</th>\n",
              "      <td>Leal J</td>\n",
              "      <td>Pseudomonas aeruginosa is the predominant pat...</td>\n",
              "      <td>Division of Molecular Pharmaceutics and Drug D...</td>\n",
              "      <td>(Leal J, Division of Molecular Pharmaceutics a...</td>\n",
              "      <td>[cystic fibrosis, tobramycin, biofilm, pegylat...</td>\n",
              "      <td>48453</td>\n",
              "      <td>1912.0</td>\n",
              "      <td>{(Smyth HDC, Division of Molecular Pharmaceuti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3540</th>\n",
              "      <td>Strubberg AM</td>\n",
              "      <td>Increased intestinal permeability is a manife...</td>\n",
              "      <td>Dalton Cardiovascular Research Center, Univers...</td>\n",
              "      <td>(Strubberg AM, Dalton Cardiovascular Research ...</td>\n",
              "      <td>[enteroid, cdc42, cystic fibrosis, permeabilit...</td>\n",
              "      <td>29041</td>\n",
              "      <td>2110.0</td>\n",
              "      <td>{(Woode RA, Dalton Cardiovascular Research Cen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3541</th>\n",
              "      <td>Choi R</td>\n",
              "      <td>&lt;h4&gt;Background&lt;/h4&gt;The genus Burkholderia inc...</td>\n",
              "      <td>Department of Medicine Division of Allergy Inf...</td>\n",
              "      <td>(Choi R, Department of Medicine Division of Al...</td>\n",
              "      <td>[]</td>\n",
              "      <td>53033</td>\n",
              "      <td>1638.0</td>\n",
              "      <td>{(Myler PJ, Department of Chemistry and Bioche...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3542 rows  8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      PI                                           Abstract  \\\n",
              "0           Anthony Bier  A Prospective, Randomized, Double-blinded, Pla...   \n",
              "1          Mina Pastagia  A Phase 2, Multi-Center, Double-Blind, Randomi...   \n",
              "2        Mary C. Drinane  Relationship Between the Development of Impair...   \n",
              "3        H. Frank Farmer  An Open-Label Phase 1 Study to Examine the Eff...   \n",
              "4     Lisa Guay-Woodford  Core A: The Hepato/Renal Fibrocystic Diseases ...   \n",
              "...                  ...                                                ...   \n",
              "3537         Greenberg J   <h4>Rationale</h4>Monitoring clinical disease...   \n",
              "3538           Diggle SP   <i>Pseudomonas aeruginosa</i> is an opportuni...   \n",
              "3539              Leal J   Pseudomonas aeruginosa is the predominant pat...   \n",
              "3540        Strubberg AM   Increased intestinal permeability is a manife...   \n",
              "3541              Choi R   <h4>Background</h4>The genus Burkholderia inc...   \n",
              "\n",
              "                                            affiliation  \\\n",
              "0     Nutrasource Pharmaceutical and Nutraceutical S...   \n",
              "1                          Armata Pharmaceuticals, Inc.   \n",
              "2                    Dartmouth-Hitchcock Medical Center   \n",
              "3                                     Covance CRU, Inc.   \n",
              "4                      Childrens National Health System   \n",
              "...                                                 ...   \n",
              "3537  Division of Respiratory Diseases, Boston Child...   \n",
              "3538  Center for Microbial Dynamics and Infection, S...   \n",
              "3539  Division of Molecular Pharmaceutics and Drug D...   \n",
              "3540  Dalton Cardiovascular Research Center, Univers...   \n",
              "3541  Department of Medicine Division of Allergy Inf...   \n",
              "\n",
              "                                                   name  \\\n",
              "0     (Anthony Bier, Nutrasource Pharmaceutical and ...   \n",
              "1         (Mina Pastagia, Armata Pharmaceuticals, Inc.)   \n",
              "2     (Mary C. Drinane, Dartmouth-Hitchcock Medical ...   \n",
              "3                  (H. Frank Farmer, Covance CRU, Inc.)   \n",
              "4     (Lisa Guay-Woodford, Childrens National Health...   \n",
              "...                                                 ...   \n",
              "3537  (Greenberg J, Division of Respiratory Diseases...   \n",
              "3538  (Diggle SP, Center for Microbial Dynamics and ...   \n",
              "3539  (Leal J, Division of Molecular Pharmaceutics a...   \n",
              "3540  (Strubberg AM, Dalton Cardiovascular Research ...   \n",
              "3541  (Choi R, Department of Medicine Division of Al...   \n",
              "\n",
              "                                                  Terms   FIPS  Community  \\\n",
              "0                                  [Placebo, Probiotic]  12086      280.0   \n",
              "1                                    [Placebo, AP-PA02]  53033      253.0   \n",
              "2                                           [Fibroscan]  33009      271.0   \n",
              "3                                              [VX-770]  12127       91.0   \n",
              "4                                                    []  11001      118.0   \n",
              "...                                                 ...    ...        ...   \n",
              "3537                                                 []  25025     1769.0   \n",
              "3538  [evolution, antibiotic resistance, population ...  13121     2072.0   \n",
              "3539  [cystic fibrosis, tobramycin, biofilm, pegylat...  48453     1912.0   \n",
              "3540  [enteroid, cdc42, cystic fibrosis, permeabilit...  29041     2110.0   \n",
              "3541                                                 []  53033     1638.0   \n",
              "\n",
              "                                                network  \n",
              "0                                                    {}  \n",
              "1                                                    {}  \n",
              "2                                                    {}  \n",
              "3                                                    {}  \n",
              "4                                                    {}  \n",
              "...                                                 ...  \n",
              "3537  {(Walt DR, Department of Chemistry, Tufts Univ...  \n",
              "3538  {(Vanderwoude J, Center for Microbial Dynamics...  \n",
              "3539  {(Smyth HDC, Division of Molecular Pharmaceuti...  \n",
              "3540  {(Woode RA, Dalton Cardiovascular Research Cen...  \n",
              "3541  {(Myler PJ, Department of Chemistry and Bioche...  \n",
              "\n",
              "[3542 rows x 8 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "281f490d-52ac-4acf-9478-55eb86cc995f",
      "metadata": {
        "id": "281f490d-52ac-4acf-9478-55eb86cc995f"
      },
      "outputs": [],
      "source": [
        "fip_finding=pd.read_csv('C:/Users/Desktop/Social network all works/work rdas/uscities.csv')\n",
        "fip_finding_dict=dict()\n",
        "lat_finding_dict=dict()\n",
        "lon_finding_dict=dict()\n",
        "lon_lag_fip_info=dict()\n",
        "for i in fip_finding.index:\n",
        "     fip_finding_dict[fip_finding['city'][i].lower()]= fip_finding['county_fips'][i]\n",
        "     lat_finding_dict[fip_finding['city'][i].lower()]= fip_finding['lat'][i]\n",
        "     lon_finding_dict[fip_finding['city'][i].lower()]= fip_finding['lng'][i]\n",
        "     lon_lag_fip_info[( round(fip_finding['lat'][i]) ,round(fip_finding['lng'][i]) )]=fip_finding['city'][i]\n",
        "for i in concatenated_data.index:\n",
        " try:\n",
        "  concatenated_data.at[i,'lat']= fip_finding[fip_finding['county_fips'].astype(str)== str(concatenated_data['FIPS'][i]) ]['lat'].iloc[0]\n",
        "  concatenated_data.at[i,'lng']= fip_finding[fip_finding['county_fips'].astype(str)== str(concatenated_data['FIPS'][i]) ]['lng'].iloc[0]\n",
        " except:\n",
        "     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a46dd51-d166-4df4-9ec6-4827bb367634",
      "metadata": {
        "id": "7a46dd51-d166-4df4-9ec6-4827bb367634",
        "outputId": "a2573416-6250-4742-da8f-ac3212caa920"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PI</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>name</th>\n",
              "      <th>Terms</th>\n",
              "      <th>Community</th>\n",
              "      <th>network</th>\n",
              "      <th>lat</th>\n",
              "      <th>lng</th>\n",
              "      <th>fip</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Anthony Bier</td>\n",
              "      <td>A Prospective, Randomized, Double-blinded, Pla...</td>\n",
              "      <td>Nutrasource Pharmaceutical and Nutraceutical S...</td>\n",
              "      <td>(Anthony Bier, Nutrasource Pharmaceutical and ...</td>\n",
              "      <td>[Placebo, Probiotic]</td>\n",
              "      <td>280.0</td>\n",
              "      <td>{}</td>\n",
              "      <td>25.7840</td>\n",
              "      <td>-80.2101</td>\n",
              "      <td>12086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mina Pastagia</td>\n",
              "      <td>A Phase 2, Multi-Center, Double-Blind, Randomi...</td>\n",
              "      <td>Armata Pharmaceuticals, Inc.</td>\n",
              "      <td>(Mina Pastagia, Armata Pharmaceuticals, Inc.)</td>\n",
              "      <td>[Placebo, AP-PA02]</td>\n",
              "      <td>253.0</td>\n",
              "      <td>{}</td>\n",
              "      <td>47.6211</td>\n",
              "      <td>-122.3244</td>\n",
              "      <td>53033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mary C. Drinane</td>\n",
              "      <td>Relationship Between the Development of Impair...</td>\n",
              "      <td>Dartmouth-Hitchcock Medical Center</td>\n",
              "      <td>(Mary C. Drinane, Dartmouth-Hitchcock Medical ...</td>\n",
              "      <td>[Fibroscan]</td>\n",
              "      <td>271.0</td>\n",
              "      <td>{}</td>\n",
              "      <td>43.6353</td>\n",
              "      <td>-72.2531</td>\n",
              "      <td>33009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>H. Frank Farmer</td>\n",
              "      <td>An Open-Label Phase 1 Study to Examine the Eff...</td>\n",
              "      <td>Covance CRU, Inc.</td>\n",
              "      <td>(H. Frank Farmer, Covance CRU, Inc.)</td>\n",
              "      <td>[VX-770]</td>\n",
              "      <td>91.0</td>\n",
              "      <td>{}</td>\n",
              "      <td>28.9050</td>\n",
              "      <td>-81.2137</td>\n",
              "      <td>12127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lisa Guay-Woodford</td>\n",
              "      <td>Core A: The Hepato/Renal Fibrocystic Diseases ...</td>\n",
              "      <td>Childrens National Health System</td>\n",
              "      <td>(Lisa Guay-Woodford, Childrens National Health...</td>\n",
              "      <td>[]</td>\n",
              "      <td>118.0</td>\n",
              "      <td>{}</td>\n",
              "      <td>38.9047</td>\n",
              "      <td>-77.0163</td>\n",
              "      <td>11001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3537</th>\n",
              "      <td>Greenberg J</td>\n",
              "      <td>&lt;h4&gt;Rationale&lt;/h4&gt;Monitoring clinical disease...</td>\n",
              "      <td>Division of Respiratory Diseases, Boston Child...</td>\n",
              "      <td>(Greenberg J, Division of Respiratory Diseases...</td>\n",
              "      <td>[]</td>\n",
              "      <td>1769.0</td>\n",
              "      <td>{(Walt DR, Department of Chemistry, Tufts Univ...</td>\n",
              "      <td>42.3188</td>\n",
              "      <td>-71.0852</td>\n",
              "      <td>25025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3538</th>\n",
              "      <td>Diggle SP</td>\n",
              "      <td>&lt;i&gt;Pseudomonas aeruginosa&lt;/i&gt; is an opportuni...</td>\n",
              "      <td>Center for Microbial Dynamics and Infection, S...</td>\n",
              "      <td>(Diggle SP, Center for Microbial Dynamics and ...</td>\n",
              "      <td>[evolution, antibiotic resistance, population ...</td>\n",
              "      <td>2072.0</td>\n",
              "      <td>{(Vanderwoude J, Center for Microbial Dynamics...</td>\n",
              "      <td>33.7628</td>\n",
              "      <td>-84.4220</td>\n",
              "      <td>13121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3539</th>\n",
              "      <td>Leal J</td>\n",
              "      <td>Pseudomonas aeruginosa is the predominant pat...</td>\n",
              "      <td>Division of Molecular Pharmaceutics and Drug D...</td>\n",
              "      <td>(Leal J, Division of Molecular Pharmaceutics a...</td>\n",
              "      <td>[cystic fibrosis, tobramycin, biofilm, pegylat...</td>\n",
              "      <td>1912.0</td>\n",
              "      <td>{(Smyth HDC, Division of Molecular Pharmaceuti...</td>\n",
              "      <td>30.3005</td>\n",
              "      <td>-97.7522</td>\n",
              "      <td>48453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3540</th>\n",
              "      <td>Strubberg AM</td>\n",
              "      <td>Increased intestinal permeability is a manife...</td>\n",
              "      <td>Dalton Cardiovascular Research Center, Univers...</td>\n",
              "      <td>(Strubberg AM, Dalton Cardiovascular Research ...</td>\n",
              "      <td>[enteroid, cdc42, cystic fibrosis, permeabilit...</td>\n",
              "      <td>2110.0</td>\n",
              "      <td>{(Woode RA, Dalton Cardiovascular Research Cen...</td>\n",
              "      <td>39.4233</td>\n",
              "      <td>-92.8025</td>\n",
              "      <td>29041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3541</th>\n",
              "      <td>Choi R</td>\n",
              "      <td>&lt;h4&gt;Background&lt;/h4&gt;The genus Burkholderia inc...</td>\n",
              "      <td>Department of Medicine Division of Allergy Inf...</td>\n",
              "      <td>(Choi R, Department of Medicine Division of Al...</td>\n",
              "      <td>[]</td>\n",
              "      <td>1638.0</td>\n",
              "      <td>{(Myler PJ, Department of Chemistry and Bioche...</td>\n",
              "      <td>47.6211</td>\n",
              "      <td>-122.3244</td>\n",
              "      <td>53033</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3542 rows  10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      PI                                           Abstract  \\\n",
              "0           Anthony Bier  A Prospective, Randomized, Double-blinded, Pla...   \n",
              "1          Mina Pastagia  A Phase 2, Multi-Center, Double-Blind, Randomi...   \n",
              "2        Mary C. Drinane  Relationship Between the Development of Impair...   \n",
              "3        H. Frank Farmer  An Open-Label Phase 1 Study to Examine the Eff...   \n",
              "4     Lisa Guay-Woodford  Core A: The Hepato/Renal Fibrocystic Diseases ...   \n",
              "...                  ...                                                ...   \n",
              "3537         Greenberg J   <h4>Rationale</h4>Monitoring clinical disease...   \n",
              "3538           Diggle SP   <i>Pseudomonas aeruginosa</i> is an opportuni...   \n",
              "3539              Leal J   Pseudomonas aeruginosa is the predominant pat...   \n",
              "3540        Strubberg AM   Increased intestinal permeability is a manife...   \n",
              "3541              Choi R   <h4>Background</h4>The genus Burkholderia inc...   \n",
              "\n",
              "                                            affiliation  \\\n",
              "0     Nutrasource Pharmaceutical and Nutraceutical S...   \n",
              "1                          Armata Pharmaceuticals, Inc.   \n",
              "2                    Dartmouth-Hitchcock Medical Center   \n",
              "3                                     Covance CRU, Inc.   \n",
              "4                      Childrens National Health System   \n",
              "...                                                 ...   \n",
              "3537  Division of Respiratory Diseases, Boston Child...   \n",
              "3538  Center for Microbial Dynamics and Infection, S...   \n",
              "3539  Division of Molecular Pharmaceutics and Drug D...   \n",
              "3540  Dalton Cardiovascular Research Center, Univers...   \n",
              "3541  Department of Medicine Division of Allergy Inf...   \n",
              "\n",
              "                                                   name  \\\n",
              "0     (Anthony Bier, Nutrasource Pharmaceutical and ...   \n",
              "1         (Mina Pastagia, Armata Pharmaceuticals, Inc.)   \n",
              "2     (Mary C. Drinane, Dartmouth-Hitchcock Medical ...   \n",
              "3                  (H. Frank Farmer, Covance CRU, Inc.)   \n",
              "4     (Lisa Guay-Woodford, Childrens National Health...   \n",
              "...                                                 ...   \n",
              "3537  (Greenberg J, Division of Respiratory Diseases...   \n",
              "3538  (Diggle SP, Center for Microbial Dynamics and ...   \n",
              "3539  (Leal J, Division of Molecular Pharmaceutics a...   \n",
              "3540  (Strubberg AM, Dalton Cardiovascular Research ...   \n",
              "3541  (Choi R, Department of Medicine Division of Al...   \n",
              "\n",
              "                                                  Terms  Community  \\\n",
              "0                                  [Placebo, Probiotic]      280.0   \n",
              "1                                    [Placebo, AP-PA02]      253.0   \n",
              "2                                           [Fibroscan]      271.0   \n",
              "3                                              [VX-770]       91.0   \n",
              "4                                                    []      118.0   \n",
              "...                                                 ...        ...   \n",
              "3537                                                 []     1769.0   \n",
              "3538  [evolution, antibiotic resistance, population ...     2072.0   \n",
              "3539  [cystic fibrosis, tobramycin, biofilm, pegylat...     1912.0   \n",
              "3540  [enteroid, cdc42, cystic fibrosis, permeabilit...     2110.0   \n",
              "3541                                                 []     1638.0   \n",
              "\n",
              "                                                network      lat       lng  \\\n",
              "0                                                    {}  25.7840  -80.2101   \n",
              "1                                                    {}  47.6211 -122.3244   \n",
              "2                                                    {}  43.6353  -72.2531   \n",
              "3                                                    {}  28.9050  -81.2137   \n",
              "4                                                    {}  38.9047  -77.0163   \n",
              "...                                                 ...      ...       ...   \n",
              "3537  {(Walt DR, Department of Chemistry, Tufts Univ...  42.3188  -71.0852   \n",
              "3538  {(Vanderwoude J, Center for Microbial Dynamics...  33.7628  -84.4220   \n",
              "3539  {(Smyth HDC, Division of Molecular Pharmaceuti...  30.3005  -97.7522   \n",
              "3540  {(Woode RA, Dalton Cardiovascular Research Cen...  39.4233  -92.8025   \n",
              "3541  {(Myler PJ, Department of Chemistry and Bioche...  47.6211 -122.3244   \n",
              "\n",
              "        fip  \n",
              "0     12086  \n",
              "1     53033  \n",
              "2     33009  \n",
              "3     12127  \n",
              "4     11001  \n",
              "...     ...  \n",
              "3537  25025  \n",
              "3538  13121  \n",
              "3539  48453  \n",
              "3540  29041  \n",
              "3541  53033  \n",
              "\n",
              "[3542 rows x 10 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_data ['fip']=concatenated_data ['FIPS']\n",
        "concatenated_data= concatenated_data.drop('FIPS' , axis=1)\n",
        "concatenated_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e05c1f8-1456-4629-9746-10576000286e",
      "metadata": {
        "id": "0e05c1f8-1456-4629-9746-10576000286e"
      },
      "outputs": [],
      "source": [
        "concatenated_data.to_csv('concatenated_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c2f1e15-d37b-4786-af02-b4a77b4fbccd",
      "metadata": {
        "id": "0c2f1e15-d37b-4786-af02-b4a77b4fbccd"
      },
      "outputs": [],
      "source": [
        "concatenated_data1 = concatenated_data  # Initialize concatenated_data1\n",
        "Node = list(set(concatenated_data1['fip'].values))  # Extract unique 'fip' values as nodes\n",
        "Edge = []\n",
        "\n",
        "# Iterate through each row in concatenated_data1\n",
        "for i in concatenated_data1.index:\n",
        "    # Ensure the 'network' value is iterable and not NaN\n",
        "    if isinstance(concatenated_data1['network'][i], (list, str)):\n",
        "        # Iterate through each element in the 'network' column for the current row\n",
        "        for j in concatenated_data1['network'][i]:\n",
        "            # Filter data where 'name' matches the current network element (j)\n",
        "            filtered_data = concatenated_data1[concatenated_data1['name'] == j]\n",
        "\n",
        "            # If filtered data is not empty, create an edge\n",
        "            if not filtered_data.empty:\n",
        "                value = filtered_data['fip'].iloc[0]  # Get the 'fip' of the first matching row\n",
        "                edge = (concatenated_data1['fip'][i], value)  # Create the edge (tuple)\n",
        "                Edge.append(edge)  # Add the edge to the list\n",
        "    else:\n",
        "        # If 'network' value is not iterable (e.g., NaN or not a list/str), skip it\n",
        "        continue\n",
        "\n",
        "# Remove duplicate edges (if any) by converting the list of edges to a set and back to a list\n",
        "Edge = list(set(Edge))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6166ff4d-53af-413a-ab0a-c43823244746",
      "metadata": {
        "id": "6166ff4d-53af-413a-ab0a-c43823244746"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from networkx.algorithms import community #This part of networkx, for community detection, needs to be imported separately.\n",
        "#G = nx.Graph()\n",
        "G=nx.DiGraph()\n",
        "G.add_nodes_from(Node)\n",
        "G.add_edges_from(Edge)\n",
        "#nx.info(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1048b3d-734b-4754-a092-58064253d051",
      "metadata": {
        "id": "d1048b3d-734b-4754-a092-58064253d051"
      },
      "outputs": [],
      "source": [
        "Node_utility=Node\n",
        "Edge_utility=[]\n",
        "Fips_utlity=Node\n",
        "Lat_utlity=[]\n",
        "Log_utlity=[]\n",
        "n=0\n",
        "while n < len(Node_utility):\n",
        "      Lat_utlity.append(  concatenated_data1[concatenated_data1['fip']==Node[n]]['lat'].iloc[0] )\n",
        "      Log_utlity.append(  concatenated_data1[concatenated_data1['fip']==Node[n]]['lng'].iloc[0] )\n",
        "      n+=1\n",
        "      #Node_utility.append(Utility_County[0][i])\n",
        "\n",
        "G_utility=nx.DiGraph()\n",
        "G_utility.add_nodes_from(Node_utility)\n",
        "G_utility.add_edges_from(Edge_utility)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "675532d9-dfe3-499d-889a-aa577e043d66",
      "metadata": {
        "id": "675532d9-dfe3-499d-889a-aa577e043d66"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "Fips_latlong_utility=pd.DataFrame()\n",
        "Fips_latlong_utility['Utility Number']=Node_utility\n",
        "Fips_latlong_utility['fips']=Node_utility\n",
        "Fips_latlong_utility['Latitude']=Lat_utlity\n",
        "Fips_latlong_utility['Longitude']=Log_utlity\n",
        "\n",
        "\n",
        "data_G=pd.DataFrame()\n",
        "data_G['Node']=Node\n",
        "d = nx.degree(G)\n",
        "data_G['degree']=d\n",
        "\n",
        "data_G_utility=pd.DataFrame()\n",
        "data_G_utility['Node']=Fips_latlong_utility['Utility Number']\n",
        "data_G_utility['degree']=''\n",
        "for i in range(data_G_utility.shape[0]):\n",
        "    if Fips_latlong_utility['fips'][i] in data_G['Node'].values:\n",
        "        data_G_utility.at[i,'degree'] = data_G.loc[data_G['Node'] == Fips_latlong_utility['fips'][i]]['degree'].item()\n",
        "    else:\n",
        "        data_G_utility.at[i,'degree'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2db8335-4cf6-4459-aa84-d6fd65c2d5c4",
      "metadata": {
        "id": "f2db8335-4cf6-4459-aa84-d6fd65c2d5c4"
      },
      "outputs": [],
      "source": [
        "for i in range(data_G_utility.shape[0]):\n",
        "     data_G_utility.at[i,'degree']=data_G_utility['degree'][i][1]\n",
        "List=np.array(data_G_utility['degree'].astype(float))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96b32788-76c4-455a-95a6-fc2715b84bcd",
      "metadata": {
        "id": "96b32788-76c4-455a-95a6-fc2715b84bcd",
        "outputId": "46cabf80-8410-4e1e-9fd7-44a3dc021771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Using cached setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
            "Installing collected packages: setuptools\n",
            "Successfully installed setuptools-75.8.0\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python313\\site-packages (2.2.1)\n",
            "Collecting cython\n",
            "  Downloading Cython-3.0.11-cp313-cp313-win_amd64.whl.metadata (3.2 kB)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.0-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.1-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.55.3-cp313-cp313-win_amd64.whl.metadata (168 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Downloading pillow-11.1.0-cp313-cp313-win_amd64.whl.metadata (9.3 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading Cython-3.0.11-cp313-cp313-win_amd64.whl (2.8 MB)\n",
            "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.8/2.8 MB 22.1 MB/s eta 0:00:00\n",
            "Downloading matplotlib-3.10.0-cp313-cp313-win_amd64.whl (8.0 MB)\n",
            "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
            "   ---------------------------------------  7.9/8.0 MB 44.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.0/8.0 MB 29.0 MB/s eta 0:00:00\n",
            "Downloading contourpy-1.3.1-cp313-cp313-win_amd64.whl (220 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.55.3-cp313-cp313-win_amd64.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.2/2.2 MB 26.8 MB/s eta 0:00:00\n",
            "Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl (71 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Downloading pillow-11.1.0-cp313-cp313-win_amd64.whl (2.6 MB)\n",
            "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.6/2.6 MB 31.6 MB/s eta 0:00:00\n",
            "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: six, pyparsing, pillow, packaging, kiwisolver, fonttools, cython, cycler, contourpy, python-dateutil, matplotlib\n",
            "Successfully installed contourpy-1.3.1 cycler-0.12.1 cython-3.0.11 fonttools-4.55.3 kiwisolver-1.4.8 matplotlib-3.10.0 packaging-24.2 pillow-11.1.0 pyparsing-3.2.1 python-dateutil-2.9.0.post0 six-1.17.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'C:\\Users\\valinejadj2\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\valinejadj2\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "!pip install setuptools\n",
        "!pip install numpy cython matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37bb93f0-dfb7-4aab-8607-165cd2b19222",
      "metadata": {
        "id": "37bb93f0-dfb7-4aab-8607-165cd2b19222",
        "outputId": "c4051882-76d3-4640-f952-a3ed35658972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'conda-forge'\n"
          ]
        }
      ],
      "source": [
        "!pip install -c conda-forge basemap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f13b1144-44c3-41c7-b8c3-c9733d1cb3d7",
      "metadata": {
        "id": "f13b1144-44c3-41c7-b8c3-c9733d1cb3d7"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.basemap import Basemap as Basemap\n",
        "plt.figure(figsize = (20,9))\n",
        "m = Basemap(\n",
        "         #projection='merc',   'mill',  'cyl'\n",
        "         llcrnrlon=-130,    #-180,  left\n",
        "         llcrnrlat=25,         #   10,  south\n",
        "         urcrnrlon=-50,   #-50, right\n",
        "         urcrnrlat=50,    #70,  North\n",
        "         lat_ts=0,\n",
        "         resolution='l',     #   c - crude,  l - low,  h - high,  f - full\n",
        "         suppress_ticks=True)\n",
        "\n",
        "mx, my = m(pd.to_numeric(fip_finding['lng'].astype(str).str.replace('', '-', regex=True), downcast='float').values,  pd.to_numeric(fip_finding['lat'], downcast='float').values)\n",
        "\n",
        "############ Counties\n",
        "#mx, my = m(pd.to_numeric(fip_finding['lng'].str.replace('', '-', regex=True), downcast='float').values,  pd.to_numeric(fip_finding['lat'], downcast='float').values)\n",
        "pos = {}\n",
        "\n",
        "fip_finding['county_fips'] = fip_finding['county_fips'].astype(str)\n",
        "fip_finding['county_fips'] = fip_finding['county_fips'].apply(lambda x: '{0:0>5}'.format(x))\n",
        "\n",
        "for count, elem in enumerate (fip_finding['county_fips'].values):\n",
        "      pos[elem] = (mx[count], my[count])\n",
        "\n",
        "\n",
        "#pos=nx.spring_layout(G)\n",
        "d = nx.degree(G)\n",
        "d = [(d[node]+1) * 25 for node in G.nodes()]\n",
        "nx.draw_networkx_nodes(G , pos = pos, node_color = 'r', alpha = 0.8, node_size=d) #node_size = 4)\n",
        "nx.draw_networkx_edges(G , pos = pos, edge_color='g',alpha=0.2, arrows = False, width=2)\n",
        "\n",
        "\n",
        "\n",
        "####### Utilities\n",
        "#mx1, my1 = m(pd.to_numeric(Fips_latlong_utility['Longitude'].str.replace('', '-', regex=True), downcast='float').values,  pd.to_numeric(Fips_latlong_utility['Latitude'], downcast='float').values)\n",
        "pos1 = {}\n",
        "#for count, elem in enumerate (Fips_latlong_utility['Utility Number'].values):\n",
        "#       pos1[elem] = (mx[count], my[count])\n",
        "n=0\n",
        "while n<len(Node_utility):\n",
        "  pos1[Node_utility[n]] = (Log_utlity[n],Lat_utlity[n])\n",
        "  n+=1\n",
        "\n",
        "dd = List*1\n",
        "nx.draw_networkx_nodes(G_utility , pos = pos1,node_shape ='s' , node_color = 'k', alpha = 0.8,node_size=1) #node_size = 4,   node_shape  'so^>v<dph8'\n",
        "#filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X')\n",
        "\n",
        "\n",
        "m.drawcountries(linewidth = 3)\n",
        "m.drawstates(linewidth = 0.2)\n",
        "m.drawcoastlines(linewidth=3)\n",
        "#m.drawcounties(color='darkred')\n",
        "#m.bluemarble()\n",
        "#m.etopo()\n",
        "\n",
        "plt.tight_layout()\n",
        "#plt.savefig(\"/content/drive/Shareddrives/MY Files/Utility-based /map_1.png\", format = \"png\", dpi = 300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82720d4b-e854-430c-a701-ccc7b89336b8",
      "metadata": {
        "id": "82720d4b-e854-430c-a701-ccc7b89336b8"
      },
      "outputs": [],
      "source": [
        "!pip install python-louvain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb7cd37-9a90-48c2-8923-01a688dd3fa2",
      "metadata": {
        "id": "3cb7cd37-9a90-48c2-8923-01a688dd3fa2"
      },
      "outputs": [],
      "source": [
        "G=nx.Graph()\n",
        "G.add_nodes_from(Node)\n",
        "G.add_edges_from(Edge)\n",
        "\n",
        "###########################################################  Cliques ###################################\n",
        "cl=nx.enumerate_all_cliques(G)\n",
        "#print last 10 cliques\n",
        "print([l for l in cl][-10:])\n",
        "#\"5 Largest Cliques\"\n",
        "print(\"5 Largest Cliques\",sorted([l for l in nx.find_cliques(G)],key=lambda x: len(x),reverse=True)[:5])\n",
        "\n",
        "##################################################### PAGERANK and HITS  ###################################\n",
        "pg_rank=sorted([l for l in nx.pagerank(G).items()],key=lambda x: x[1],reverse=True)\n",
        "print(\"Top 10 county By Pagerank\",pg_rank[:10])\n",
        "\n",
        "hubs,authorities=nx.hits(G)\n",
        "hubs=sorted([l for l in hubs.items()],key=lambda x: x[1],reverse=True)\n",
        "authorities=sorted([l for l in authorities.items()],key=lambda x: x[1],reverse=True)\n",
        "print(\"Top 10 Biggest Hubs\",hubs[:10])\n",
        "print(\"\\nTop 10 Biggest Authorities\",authorities[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02600ceb-fdae-439e-a550-d6579fb989ea",
      "metadata": {
        "id": "02600ceb-fdae-439e-a550-d6579fb989ea"
      },
      "outputs": [],
      "source": [
        "############################  Communty Detection  ######################\n",
        "import community #requires separate install -  pip install python-louvain\n",
        "from community import community_louvain\n",
        "import warnings\n",
        "\n",
        "plt.figure(figsize=(20,9))\n",
        "#pos = nx.spring_layout(Gc,iterations=50,k=2)\n",
        "partition = community_louvain.best_partition(G)\n",
        "values = [partition.get(node) for node in G.nodes()]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.title('Social Network'.format(len(set(partition.values()))))\n",
        "#nx.draw_networkx(Gc, pos = pos, cmap = plt.get_cmap(\"jet\"), node_color = values, node_size = 1, with_labels = False)\n",
        "\n",
        "\n",
        "nx.draw_networkx_nodes(G , pos = pos, cmap = plt.get_cmap(\"jet\"), node_color = values, alpha = 0.8, node_size=50) #node_size = 4)\n",
        "nx.draw_networkx_edges(G , pos = pos, edge_color='k',alpha=0.2, arrows = False,width=5)\n",
        "\n",
        "\n",
        "m.drawcountries(linewidth = 3)\n",
        "m.drawstates(linewidth = 0.2)\n",
        "m.drawcoastlines(linewidth=3)\n",
        "#m.drawcounties(color='darkred')\n",
        "#m.bluemarble()\n",
        "#m.etopo()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "744ca6c6-6b39-4597-bda9-b24fecb2b58c",
      "metadata": {
        "id": "744ca6c6-6b39-4597-bda9-b24fecb2b58c"
      },
      "outputs": [],
      "source": [
        "L = nx.laplacian_matrix(G)\n",
        "eig_values, eig_vectors = np.linalg.eigh(L.todense()) # Eigen values sorted from smallest to biggest\n",
        "f = eig_vectors[:,1] # use the second smallest Eigen vector for spectral clustering\n",
        "labels = np.ravel(np.sign(f)) # decides the label of each node\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "\n",
        "fig = plt.figure(figsize=(9,6))\n",
        "plt.axis(\"off\")\n",
        "#nx.draw_networkx(Gc, pos,node_size=45, cmap = plt.get_cmap(\"jet\"), node_color=labels, with_labels = False)\n",
        "nx.draw_networkx_nodes(G , pos = pos, cmap = plt.get_cmap(\"jet\"), node_color = labels, alpha = 0.8, node_size=10) #node_size = 4)\n",
        "nx.draw_networkx_edges(G , pos = pos, edge_color='k',alpha=0.2, arrows = False)\n",
        "\n",
        "\n",
        "m.drawcountries(linewidth = 3)\n",
        "m.drawstates(linewidth = 0.2)\n",
        "m.drawcoastlines(linewidth=3)\n",
        "#m.drawcounties(color='darkred')\n",
        "#m.bluemarble()\n",
        "#m.etopo()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5840c1d-c21f-43af-83d5-a1bae4f0f223",
      "metadata": {
        "id": "e5840c1d-c21f-43af-83d5-a1bae4f0f223"
      },
      "source": [
        "# Clustering new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241b9da1-f627-45d5-8961-48562117cd42",
      "metadata": {
        "id": "241b9da1-f627-45d5-8961-48562117cd42"
      },
      "outputs": [],
      "source": [
        "def merge_community_detection_and_researchers_info(disease):\n",
        " with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "  with driver.session() as session:\n",
        "   query = \"MATCH (i:ClinicalTrial)--(g:Gard) WHERE g.GARDname= $disease RETURN i.OfficialName as OfficialName , i.OfficialAffiliation as OfficialAffiliation, i.OfficialTitle as OfficialTitle  ,i.Interventions as concatenatedInterventions, i.BriefSummary as BriefSummary\"\n",
        "   results = session.run(query, disease=disease)\n",
        "   data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "   if data==[]:  ClinicalTrial = pd.DataFrame(columns=[\"OfficialName\", \"OfficialAffiliation\", \"OfficialTitle\", \"concatenatedInterventions\", \"BriefSummary\"])\n",
        "   else: ClinicalTrial = pd.DataFrame(data)\n",
        "   ######   Grant\n",
        "   query = \"MATCH (i:Grant)--(g:Gard) WHERE g.GARDname= $disease RETURN i.pi_name as pi_name,i.org_name as org_name,i.title as title ,i.terms as terms,i.abstract as abstract\"\n",
        "   results = session.run(query, disease=disease)\n",
        "   data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "   Grant = pd.DataFrame(data)\n",
        "   ##################### pubmed\n",
        "   query = \"MATCH (i:Pubmed)--(g:Gard) WHERE g.GARDname= $disease RETURN i.fullName as fullName, i.ar_affiliation as affiliation, i.title as title, i.concatenatedKeywords as concatenatedKeywords,i.abstractText as abstractText \"\n",
        "   results = session.run(query, disease=disease)\n",
        "   data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "   Pubmed = pd.DataFrame(data)\n",
        "   ################\n",
        "   ClinicalTrial['name'] = ClinicalTrial.apply(lambda row: (row['OfficialName'], row['OfficialAffiliation']), axis=1)\n",
        "   Grant['name'] = Grant.apply(lambda row: (row['pi_name'], row['org_name']), axis=1)\n",
        "   Pubmed['name'] = Pubmed.apply(lambda row: (row['fullName'], row['affiliation']), axis=1)\n",
        "\n",
        "   ClinicalTrial['BriefSummary']=ClinicalTrial['OfficialTitle'].fillna(\"\").astype(str)+' '+ ClinicalTrial['BriefSummary'].fillna(\"\").astype(str)   #+ClinicalTrial['concatenatedInterventions'].fillna(\"\").astype(str)+\n",
        "   Grant['BriefSummary']=Grant['title'].fillna(\"\").astype(str)+' '+ Grant['abstract'].fillna(\"\").astype(str)                   #+Grant['terms'].fillna(\"\").astype(str)\n",
        "   Pubmed['abstractText']=Pubmed['title'].fillna(\"\").astype(str)+' '+ Pubmed['abstractText'].fillna(\"\").astype(str)    #+Pubmed['concatenatedKeywords'].fillna(\"\").astype(str)\n",
        "\n",
        "   ClinicalTrial['Terms']=ClinicalTrial['concatenatedInterventions']\n",
        "   Grant['Terms']=Grant['terms']\n",
        "   Pubmed['Terms']=Pubmed['concatenatedKeywords']\n",
        "   #df1=ClinicalTrial[['i.OfficialName',  'c.BriefSummary', 'i.OfficialAffiliation','name']]\n",
        "   df1=ClinicalTrial[['OfficialName',  'BriefSummary','OfficialAffiliation', 'name','Terms']]\n",
        "   df2=Grant[['pi_name', 'abstract', 'org_name','name','Terms']]\n",
        "   df3=Pubmed[['fullName',  'abstractText', 'affiliation','name','Terms']]\n",
        "   # Rename columns to avoid conflicts\n",
        "   df1.columns = ['PI','Abstract', 'affiliation','name','Terms']\n",
        "   df2.columns = ['PI','Abstract', 'affiliation','name','Terms']\n",
        "   df3.columns = ['PI','Abstract', 'affiliation','name','Terms']\n",
        "   #df2['Location']=  df2['locationCity'] +' , '+df2['Location']\n",
        "   result_df = pd.concat([df1  , df2, df3])\n",
        "   #result_df = pd.concat([df1,  df3])\n",
        "   result_df=result_df.drop_duplicates()\n",
        "   result_df.index= range(result_df.shape[0])\n",
        "   #################################################### merging with c\n",
        "   '''\n",
        "  merged_data1 = pd.merge(result_df,data_table , left_on='name', right_on='Node', how='outer')\n",
        "  concatenated_data = merged_data1.drop_duplicates(subset=['name'])\n",
        "  concatenated_data.reset_index(drop=True, inplace=True)\n",
        "  concatenated_data=concatenated_data.drop(['name'],axis=1)\n",
        "  return concatenated_data\n",
        "   '''\n",
        "   return result_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a08ecc-dc6d-4f1a-bddf-f62ce646750f",
      "metadata": {
        "id": "c7a08ecc-dc6d-4f1a-bddf-f62ce646750f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def sum_strings(series):\n",
        "    # Convert all items to string, handling NaNs, and remove duplicates by converting to a set\n",
        "    return ', '.join(sorted(set(str(item) for item in series if pd.notna(item))))\n",
        "\n",
        "# Function to clean and flatten terms\n",
        "def sum_terms(series):\n",
        "    result = []\n",
        "    for item in series:\n",
        "        if isinstance(item, list):  # Check if item is a list\n",
        "            for subitem in item:\n",
        "                if isinstance(subitem, str):  # Ensure it's a string\n",
        "                    subitem_cleaned = subitem.strip('[]')  # Remove surrounding brackets\n",
        "                    result.extend(subitem_cleaned.split(','))  # Split and add terms\n",
        "        elif pd.notna(item):  # Handle non-list, non-NaN values\n",
        "            item_cleaned = str(item).strip('[]')  # Convert to string and remove brackets\n",
        "            result.extend(item_cleaned.split(','))  # Split and add terms\n",
        "    # Remove extra whitespace from terms\n",
        "    return [term.strip() for term in result if term.strip()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7213ea-7a2c-4084-9cc1-6bf213bf750c",
      "metadata": {
        "id": "7e7213ea-7a2c-4084-9cc1-6bf213bf750c",
        "outputId": "7a6b7c95-3f4b-4b2b-f04f-6a1b751d2b84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tobramycin for Inhalation , Tobramycin solution for inhalation (TOBI), Oral placebo, Oral ciprofloxacin\n"
          ]
        }
      ],
      "source": [
        "import ast\n",
        "from collections import Counter\n",
        "def three_most_frequent_terms(term_string):\n",
        "    # Convert the string representation of the list to an actual list\n",
        "    #term_list = ast.literal_eval(term_string)\n",
        "    # Count the frequency of each term in the list\n",
        "    term_counts = Counter(term_string)\n",
        "    # Get the three most common terms\n",
        "    most_common_terms = term_counts.most_common(5)\n",
        "\n",
        "    return ' , '.join([term  for term, count in most_common_terms])\n",
        "\n",
        "# Example usage\n",
        "terms = [\n",
        "    'Tobramycin for Inhalation',\n",
        "    'Tobramycin solution for inhalation (TOBI), Oral placebo, Oral ciprofloxacin',\n",
        "    'Tobramycin for Inhalation',\n",
        "    'Tobramycin for Inhalation',\n",
        "    'Tobramycin for Inhalation',\n",
        "    'Tobramycin for Inhalation',\n",
        "    'Tobramycin for Inhalation',\n",
        "    'Tobramycin for Inhalation',\n",
        "    'Tobramycin for Inhalation',\n",
        "    'Tobramycin solution for inhalation (TOBI), Oral placebo, Oral ciprofloxacin',\n",
        "    'Tobramycin solution for inhalation (TOBI), Oral placebo, Oral ciprofloxacin']\n",
        "\n",
        "print(three_most_frequent_terms(terms))\n",
        "#data['frequent_words']  = data.apply(lambda x:  three_most_frequent_terms(x['Terms'])   , axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f96bc6-9dee-4284-b5a3-a0a8925f2197",
      "metadata": {
        "id": "75f96bc6-9dee-4284-b5a3-a0a8925f2197",
        "outputId": "854cbc04-761e-41be-817f-e2687b4c2f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41061\n",
            "------------------\n",
            "37643\n",
            "3091\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "3\n",
            "1\n",
            "5\n",
            "4\n",
            "7\n",
            "2\n",
            "2\n",
            "2\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "2\n",
            "1\n",
            "6\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "3\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "2\n",
            "1\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def name_clusters(df, text_column, cluster_column, eps=0.5, min_samples=5, n_components=100):\n",
        "    # Extract abstracts from the specified column and handle NaN values\n",
        "    abstracts = df[text_column].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "    # TF-IDF vectorization with ngram_range (optimize based on your dataset)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))  # Bi-grams may help for better context\n",
        "    tfidf_matrix = vectorizer.fit_transform(abstracts)\n",
        "\n",
        "    # Optionally reduce dimensionality to speed up clustering\n",
        "    # if n_components:\n",
        "    #     svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "    #     tfidf_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # DBSCAN clustering\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')  # Using cosine distance as the metric\n",
        "    clusters = dbscan.fit_predict(tfidf_matrix)\n",
        "\n",
        "    # Assign cluster labels to the DataFrame\n",
        "    df[cluster_column] = clusters+2\n",
        "\n",
        "    return df\n",
        "\n",
        "disease= 'Cystic fibrosis'  #'Actinic prurigo' #'Non-syndromic polydactyly'   #'Cystic fibrosis'\n",
        "#diseaseid=B[0] #'GARD0017510'\n",
        "\n",
        "concatenated_data= merge_community_detection_and_researchers_info(disease)\n",
        "# Assuming you have a DataFrame `clustering_2`\n",
        "clustering_ = name_clusters(concatenated_data, 'Abstract', 'Community', eps=0.9, min_samples=10, n_components=100)\n",
        "# Group by 'Community' and aggregate columns using sum_strings and sum_terms functions\n",
        "result = clustering_.groupby('Community').agg(\n",
        "    Abstract=('Abstract', sum_strings),\n",
        "    Terms=('Terms', sum_terms),\n",
        "    Size=('Community', 'size')  # Count of occurrences\n",
        ").reset_index()\n",
        "\n",
        "data=result\n",
        "\n",
        "print(len(clustering_['Abstract'].unique()))\n",
        "print('------------------')\n",
        "for i in clustering_['Community'].unique():\n",
        "    ASD= clustering_[clustering_['Community']==i]\n",
        "    print( ASD['Abstract'].drop_duplicates().shape[0] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da94362-e90f-4903-be10-a8e12ccbbb8b",
      "metadata": {
        "id": "9da94362-e90f-4903-be10-a8e12ccbbb8b"
      },
      "outputs": [],
      "source": [
        "def convert_to_list(text):\n",
        "    # Remove the surrounding square brackets and strip any leading/trailing whitespaces\n",
        "    text=text.replace('<|end_of_text|>','')\n",
        "    text = text.strip(\"[]\")\n",
        "    # Split by commas to get individual elements\n",
        "    words = text.split(',')\n",
        "    # Clean each word by stripping extra spaces and quotes\n",
        "    cleaned_words = [word.strip().strip(\"'\\\"\")  for word in words   if word.strip().strip(\"'\\\"\")  !='']\n",
        "    return cleaned_words\n",
        "import ast\n",
        "\n",
        "def str_to_dict(s):\n",
        "    try:\n",
        "        # Use ast.literal_eval to safely evaluate the string as a dictionary\n",
        "        return ast.literal_eval(s)\n",
        "    except (ValueError, SyntaxError) as e:\n",
        "        print(f\"Error parsing string: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "str_dict = \"{'key1': 'value1', 'key2': 'value2'}\"\n",
        "converted_dict = str_to_dict(str_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dbb8a03-79fc-4f92-a329-71226397785f",
      "metadata": {
        "id": "5dbb8a03-79fc-4f92-a329-71226397785f"
      },
      "outputs": [],
      "source": [
        "data['frequent_words']  = data.apply(lambda x:  three_most_frequent_terms(x['Terms'])   , axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317c9f25-dc1d-463a-9f88-b977a0990f56",
      "metadata": {
        "id": "317c9f25-dc1d-463a-9f88-b977a0990f56"
      },
      "outputs": [],
      "source": [
        "def create_Cluster_node(properties, disease):\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            # Query to create a Cluster node and link it to the Gard node via a relationship\n",
        "            query = (\n",
        "                \"MATCH (g:Gard {GARDname: $disease}) \"  # Match the Gard node with GARDname = $disease\n",
        "                \"CREATE (Cl:Cluster {\"\n",
        "                'Cluster_ID: $Cluster_ID,'\n",
        "                'Cluster_Size: $Cluster_Size,'\n",
        "                'Evidence: $Evidence,'\n",
        "                'Key_trems: $Key_trems'\n",
        "                \"}) \"\n",
        "                \"CREATE (Cl)-[:Cluster_to_GARD]->(g)\"  # Create the relationship between Gard and Cluster\n",
        "            )\n",
        "            session.run(query, **properties, disease=disease)\n",
        "\n",
        "            # Create Expertise nodes for the current cluster if applicable\n",
        "            #if properties['Llama_3_n']:  # If 'Key_trems' exists in properties\n",
        "            #      dic_expertise=str_to_dict(properties['Llama_3_n'])# Convert to list of expertise terms\n",
        "            #      for sentence in dic_expertise:\n",
        "            #           if (sentence!= '') and (sentence.lower() != disease.lower()) and (dic_expertise[sentence]!=''):\n",
        "            #                          create_expertise_node(session, sentence, dic_expertise[sentence],properties['Cluster_ID'], disease)\n",
        "\n",
        "\n",
        "\n",
        "def create_expertise_node(session, sentence,explan, cluster_id, disease):\n",
        "    # Create a new Expertise node with a relationship to the Cluster\n",
        "    query = (\n",
        "        \"MERGE (e:Expertise {Summarized_expertise: $sentence, expertise_explanation:$explan}) \"\n",
        "        \"WITH e \"\n",
        "        \"MATCH (c:Cluster)--(g:Gard) \"\n",
        "        \"WHERE c.Cluster_ID = $cluster_id AND g.GARDname = $disease \"\n",
        "        \"CREATE (c)-[:HAS_EXPERTISE ]->(e)\"\n",
        "    )\n",
        "    session.run(query, sentence=sentence, explan=explan,cluster_id=cluster_id, disease=disease)\n",
        "\n",
        "\n",
        "# Iterate over data to create Cluster and Expertise nodes\n",
        "for i in data.index:\n",
        "    event_properties = {\n",
        "        'Cluster_ID': str(data['Community'][i]),\n",
        "        'Cluster_Size': str(data['Size'][i]),\n",
        "        'Evidence': str(data['Abstract'][i]),\n",
        "        'Key_trems': str(data['frequent_words'][i]),\n",
        "        'Llama_3_n': '',#data['Llama_3_n'][i],\n",
        "    }\n",
        "    create_Cluster_node(event_properties, disease)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f347c35-c88d-437b-a19d-45fba19ea340",
      "metadata": {
        "id": "6f347c35-c88d-437b-a19d-45fba19ea340"
      },
      "outputs": [],
      "source": [
        "def create_relationship_cluster_researcher(community, name, affiliation, disease):\n",
        "    \"\"\"\n",
        "    Creates a relationship between Cluster and Researcher nodes in Neo4j only if\n",
        "    the Cluster node is already connected to the Gard node.\n",
        "    \"\"\"\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            query = (\n",
        "                \"MATCH (n1:Cluster)--(i:Gard), (n2:Researcher) \"\n",
        "                \"WHERE i.GARDname = $disease AND n1.Cluster_ID = $community \"\n",
        "                \"AND n2.Name = $name AND n2.Aff_name = $affiliation \"\n",
        "                \"CREATE (n1)-[:Research_on]->(n2)\"\n",
        "            )\n",
        "            session.run(query, community=community, name=name, affiliation=affiliation, disease=disease)\n",
        "\n",
        "for i in clustering_.index:\n",
        "    # Check if node_value is a tuple/list, otherwise skip or handle it differently\n",
        "        create_relationship_cluster_researcher(\n",
        "            str(clustering_['Community'][i]),\n",
        "            clustering_['PI'][i],  # Researcher name\n",
        "            clustering_['affiliation'][i],  # Affiliation\n",
        "            disease  # Disease value (make sure this exists in your DataFrame)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "555aa8ac-04b2-4121-8501-9891f0219434",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "555aa8ac-04b2-4121-8501-9891f0219434"
      },
      "source": [
        "# The rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411a8cd4-c53b-4f53-84a2-17790fe18cb0",
      "metadata": {
        "id": "411a8cd4-c53b-4f53-84a2-17790fe18cb0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def create_relationship_between_Researchers(n1_Name, n1_Aff_name, n2_Name, n2_Aff_name, num_colab):\n",
        "   with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "    with driver.session() as session:\n",
        "        # Check if the relationship already exists in either direction\n",
        "        check_query = (\n",
        "            \"MATCH (n1:Researcher)-[r:Collaborated_With]->(n2:Researcher) \"\n",
        "            \"WHERE (n1.Name = $n1_Name AND n1.Aff_name = $n1_Aff_name AND n2.Name = $n2_Name AND n2.Aff_name = $n2_Aff_name) \"\n",
        "            \"OR (n2.Name = $n1_Name AND n2.Aff_name = $n1_Aff_name AND n1.Name = $n2_Name AND n1.Aff_name = $n2_Aff_name) \"\n",
        "            \"RETURN COUNT(r) AS relationship_count\"\n",
        "        )\n",
        "\n",
        "        result = session.run(check_query, n1_Name=n1_Name, n1_Aff_name=n1_Aff_name, n2_Name=n2_Name, n2_Aff_name=n2_Aff_name)\n",
        "        relationship_count = result.single()[\"relationship_count\"]\n",
        "\n",
        "        # If the relationship doesn't exist, create it\n",
        "        if relationship_count == 0:\n",
        "            create_query = (\n",
        "                \"MATCH (n1:Researcher), (n2:Researcher) \"\n",
        "                \"WHERE n1.Name = $n1_Name AND n1.Aff_name = $n1_Aff_name \"\n",
        "                \"AND n2.Name = $n2_Name AND n2.Aff_name = $n2_Aff_name \"\n",
        "                \"CREATE (n1)-[:Collaborated_With {num_colab: $num_colab}]->(n2)\"\n",
        "            )\n",
        "            session.run(create_query, n1_Name=n1_Name, n1_Aff_name=n1_Aff_name, n2_Name=n2_Name, n2_Aff_name=n2_Aff_name, num_colab=num_colab)\n",
        "\n",
        "\n",
        "\n",
        "for key in result_dict1:\n",
        "    create_relationship_between_Researchers(key[0][0], key[0][1], key[1][0], key[1][1], result_dict1[key])\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d031c649-bbc9-43dd-b7a7-fe7bdfd33964",
      "metadata": {
        "id": "d031c649-bbc9-43dd-b7a7-fe7bdfd33964",
        "outputId": "34178834-23ec-4213-8d46-64a6b857586d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: python-louvain in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (0.16)\n",
            "Requirement already satisfied: networkx in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from python-louvain) (3.2.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\valinejadj2\\appdata\\roaming\\python\\python312\\site-packages (from python-louvain) (1.26.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-louvain\n",
        "import networkx as nx\n",
        "layout_algorithms = [\n",
        "    nx.circular_layout,\n",
        "    nx.random_layout,\n",
        "    nx.spring_layout,\n",
        "    nx.spectral_layout,\n",
        "    nx.shell_layout,\n",
        "    nx.bipartite_layout,\n",
        "    nx.planar_layout,\n",
        "    nx.fruchterman_reingold_layout,\n",
        "    nx.kamada_kawai_layout\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "651eb92b-c75c-4231-8921-624fa3c004b4",
      "metadata": {
        "id": "651eb92b-c75c-4231-8921-624fa3c004b4"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import community\n",
        "from community import community_louvain\n",
        "import matplotlib.pyplot as plt\n",
        "# Create a directed graph\n",
        "G = nx.Graph()\n",
        "# Add nodes and edges from the dictionary\n",
        "for nodes, edges in result_dict.items():\n",
        "    G.add_node(nodes)\n",
        "    for edge in edges:\n",
        "        G.add_node(edge)\n",
        "        G.add_edge(nodes, edge)\n",
        "\n",
        "# Detect communities using Louvain method\n",
        "partition = community_louvain.best_partition(G)\n",
        "data_table = pd.DataFrame(partition.items(), columns=['Node', 'Community'])\n",
        "data_table['network']=data_table.apply(lambda x: result_dict[x['Node']] ,axis=1)\n",
        "index_community_detection= list(set(data_table[data_table['network']!=set()]['Community'].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab1e9d9-c813-407f-b5bc-f9afd7651833",
      "metadata": {
        "id": "8ab1e9d9-c813-407f-b5bc-f9afd7651833",
        "outputId": "1f67b589-5fec-4ec8-d01c-8620ebc54914"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Node</th>\n",
              "      <th>Community</th>\n",
              "      <th>network</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>(nan, nan)</td>\n",
              "      <td>14</td>\n",
              "      <td>{(nan, nan)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>(Dana Hardin, nan)</td>\n",
              "      <td>15</td>\n",
              "      <td>{(Dana Hardin, nan)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>(Jeffrey Wagener, The Childrens Hospital)</td>\n",
              "      <td>19</td>\n",
              "      <td>{(Michael Konstan, Rainbow Babies and Children...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>(Michael Konstan, Rainbow Babies and Childrens...</td>\n",
              "      <td>19</td>\n",
              "      <td>{(Peter Hiatt, Baylor College of Medicine), (R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>(Peter Hiatt, Baylor College of Medicine)</td>\n",
              "      <td>19</td>\n",
              "      <td>{(Michael Konstan, Rainbow Babies and Children...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103607</th>\n",
              "      <td>(Soergel P, NaN)</td>\n",
              "      <td>15137</td>\n",
              "      <td>{(Dauber J, NaN), (Peterson K, NaN), (Zeevi A,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103608</th>\n",
              "      <td>(Meyer K, NaN)</td>\n",
              "      <td>15137</td>\n",
              "      <td>{(Dauber J, NaN), (Peterson K, NaN), (Zeevi A,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103609</th>\n",
              "      <td>(Love R, NaN)</td>\n",
              "      <td>15137</td>\n",
              "      <td>{(Dauber J, NaN), (Peterson K, NaN), (Zeevi A,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103610</th>\n",
              "      <td>(Cornwell R, NaN)</td>\n",
              "      <td>15137</td>\n",
              "      <td>{(Dauber J, NaN), (Peterson K, NaN), (Zeevi A,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103622</th>\n",
              "      <td>(nan, NaN)</td>\n",
              "      <td>7378</td>\n",
              "      <td>{(nan, NaN)}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>81750 rows  3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     Node  Community  \\\n",
              "14                                             (nan, nan)         14   \n",
              "15                                     (Dana Hardin, nan)         15   \n",
              "19              (Jeffrey Wagener, The Childrens Hospital)         19   \n",
              "20      (Michael Konstan, Rainbow Babies and Childrens...         19   \n",
              "21              (Peter Hiatt, Baylor College of Medicine)         19   \n",
              "...                                                   ...        ...   \n",
              "103607                                   (Soergel P, NaN)      15137   \n",
              "103608                                     (Meyer K, NaN)      15137   \n",
              "103609                                      (Love R, NaN)      15137   \n",
              "103610                                  (Cornwell R, NaN)      15137   \n",
              "103622                                         (nan, NaN)       7378   \n",
              "\n",
              "                                                  network  \n",
              "14                                           {(nan, nan)}  \n",
              "15                                   {(Dana Hardin, nan)}  \n",
              "19      {(Michael Konstan, Rainbow Babies and Children...  \n",
              "20      {(Peter Hiatt, Baylor College of Medicine), (R...  \n",
              "21      {(Michael Konstan, Rainbow Babies and Children...  \n",
              "...                                                   ...  \n",
              "103607  {(Dauber J, NaN), (Peterson K, NaN), (Zeevi A,...  \n",
              "103608  {(Dauber J, NaN), (Peterson K, NaN), (Zeevi A,...  \n",
              "103609  {(Dauber J, NaN), (Peterson K, NaN), (Zeevi A,...  \n",
              "103610  {(Dauber J, NaN), (Peterson K, NaN), (Zeevi A,...  \n",
              "103622                                       {(nan, NaN)}  \n",
              "\n",
              "[81750 rows x 3 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_table[data_table['network']!=set()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a6af70-4d58-4848-adea-09bf6f5e908b",
      "metadata": {
        "id": "c9a6af70-4d58-4848-adea-09bf6f5e908b"
      },
      "outputs": [],
      "source": [
        "def merge_community_detection_and_researchers_info(data_table):\n",
        " with driver.session() as session:\n",
        "  query = \"MATCH (i:ClinicalTrial) RETURN i.OfficialName as OfficialName , i.OfficialAffiliation as OfficialAffiliation, i.OfficialTitle as OfficialTitle  ,i.concatenatedInterventions as concatenatedInterventions, i.BriefSummary as BriefSummary\"\n",
        "  results = session.run(query)\n",
        "  data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "  ClinicalTrial = pd.DataFrame(data)\n",
        "  ######   Grant\n",
        "  query = \"MATCH (i:Grant) RETURN i.pi_name as pi_name,i.org_name as org_name,i.title as title ,i.terms as terms,i.abstract as abstract\"\n",
        "  results = session.run(query)\n",
        "  data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "  Grant = pd.DataFrame(data)\n",
        "  ##################### pubmed\n",
        "  query = \"MATCH (i:Pubmed) RETURN i.fullName as fullName, i.affiliation as affiliation, i.title as title, i.concatenatedKeywords as concatenatedKeywords,i.concatenatedMeshterms as concatenatedMeshterms,i.abstractText as abstractText \"\n",
        "  results = session.run(query)\n",
        "  data = [record.data() for record in results]  # Convert result to a list of dictionaries\n",
        "  Pubmed = pd.DataFrame(data)\n",
        "  ################\n",
        "  ClinicalTrial['name'] = ClinicalTrial.apply(lambda row: (row['OfficialName'], row['OfficialAffiliation']), axis=1)\n",
        "  Grant['name'] = Grant.apply(lambda row: (row['pi_name'], row['org_name']), axis=1)\n",
        "  Pubmed['name'] = Pubmed.apply(lambda row: (row['fullName'], row['affiliation']), axis=1)\n",
        "  ClinicalTrial['BriefSummary']=ClinicalTrial['OfficialTitle']+ClinicalTrial['concatenatedInterventions']+ClinicalTrial['BriefSummary']\n",
        "  Grant['BriefSummary']=Grant['title']+Grant['terms']+Grant['abstract']\n",
        "  Pubmed['abstractText']=Pubmed['title']+Pubmed['concatenatedKeywords']+Pubmed['concatenatedMeshterms']+Pubmed['abstractText']\n",
        "  ClinicalTrial['Terms']=ClinicalTrial['concatenatedInterventions']\n",
        "  Grant['Terms']=Grant['terms']\n",
        "  Pubmed['Terms']=Pubmed['concatenatedKeywords']+Pubmed['concatenatedMeshterms']\n",
        "  #df1=ClinicalTrial[['i.OfficialName',  'c.BriefSummary', 'i.OfficialAffiliation','name']]\n",
        "  df1=ClinicalTrial[['OfficialName',  'BriefSummary','OfficialAffiliation', 'name','Terms']]\n",
        "  df2=Grant[['pi_name', 'abstract', 'org_name','name','Terms']]\n",
        "  df3=Pubmed[['fullName',  'abstractText', 'affiliation','name','Terms']]\n",
        "  # Rename columns to avoid conflicts\n",
        "  df1.columns = ['PI','Abstract', 'affiliation','name','Terms']\n",
        "  df2.columns = ['PI','Abstract', 'affiliation','name','Terms']\n",
        "  df3.columns = ['PI','Abstract', 'affiliation','name','Terms']\n",
        "  #df2['Location']=  df2['locationCity'] +' , '+df2['Location']\n",
        "  result_df = pd.concat([df1  , df2, df3])\n",
        "  #result_df = pd.concat([df1,  df3])\n",
        "  result_df=result_df.drop_duplicates()\n",
        "  #################################################### merging with c\n",
        "  merged_data1 = pd.merge(result_df,data_table , left_on='name', right_on='Node', how='outer')\n",
        "  concatenated_data = merged_data1.drop_duplicates(subset=['name'])\n",
        "  concatenated_data.reset_index(drop=True, inplace=True)\n",
        "  concatenated_data=concatenated_data.drop(['name'],axis=1)\n",
        "  return concatenated_data\n",
        "concatenated_data= merge_community_detection_and_researchers_info(data_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb31971-0162-4578-953d-97ab83341bd9",
      "metadata": {
        "id": "4fb31971-0162-4578-953d-97ab83341bd9",
        "outputId": "db0989d2-9bb1-45d4-c9eb-5ffb735710dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PI</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>Terms</th>\n",
              "      <th>Node</th>\n",
              "      <th>Community</th>\n",
              "      <th>network</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>(nan, nan)</td>\n",
              "      <td>14.0</td>\n",
              "      <td>{(nan, nan)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dearbhla Hull</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Zambon SpA</td>\n",
              "      <td>None</td>\n",
              "      <td>(Dearbhla Hull, Zambon SpA)</td>\n",
              "      <td>749.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Anthony Bier</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Nutrasource Pharmaceutical and Nutraceutical S...</td>\n",
              "      <td>None</td>\n",
              "      <td>(Anthony Bier, Nutrasource Pharmaceutical and ...</td>\n",
              "      <td>1073.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mina Pastagia</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Armata Pharmaceuticals, Inc.</td>\n",
              "      <td>None</td>\n",
              "      <td>(Mina Pastagia, Armata Pharmaceuticals, Inc.)</td>\n",
              "      <td>958.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mary C. Drinane</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Dartmouth-Hitchcock Medical Center</td>\n",
              "      <td>None</td>\n",
              "      <td>(Mary C. Drinane, Dartmouth-Hitchcock Medical ...</td>\n",
              "      <td>1043.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96592</th>\n",
              "      <td>Eiberg, H., Schmiegelow, K., Tsui, L.-C., Buch...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96593</th>\n",
              "      <td>Eiberg, H., Mohr, J., Nielsen, L. S.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96594</th>\n",
              "      <td>Danks, D. M., Allan, J., Anderson, C. M.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96595</th>\n",
              "      <td>Cutting, G. R.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96596</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Chu Q, Genzyme Corporation, Framingham, MA 01...</td>\n",
              "      <td>3595.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>96597 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      PI Abstract  \\\n",
              "0                                                    NaN      NaN   \n",
              "1                                          Dearbhla Hull      NaN   \n",
              "2                                           Anthony Bier      NaN   \n",
              "3                                          Mina Pastagia      NaN   \n",
              "4                                        Mary C. Drinane      NaN   \n",
              "...                                                  ...      ...   \n",
              "96592  Eiberg, H., Schmiegelow, K., Tsui, L.-C., Buch...      NaN   \n",
              "96593               Eiberg, H., Mohr, J., Nielsen, L. S.      NaN   \n",
              "96594           Danks, D. M., Allan, J., Anderson, C. M.      NaN   \n",
              "96595                                     Cutting, G. R.      NaN   \n",
              "96596                                                NaN      NaN   \n",
              "\n",
              "                                             affiliation Terms  \\\n",
              "0                                                    NaN  None   \n",
              "1                                             Zambon SpA  None   \n",
              "2      Nutrasource Pharmaceutical and Nutraceutical S...  None   \n",
              "3                           Armata Pharmaceuticals, Inc.  None   \n",
              "4                     Dartmouth-Hitchcock Medical Center  None   \n",
              "...                                                  ...   ...   \n",
              "96592                                               None   NaN   \n",
              "96593                                               None   NaN   \n",
              "96594                                               None   NaN   \n",
              "96595                                               None   NaN   \n",
              "96596                                                NaN   NaN   \n",
              "\n",
              "                                                    Node  Community  \\\n",
              "0                                             (nan, nan)       14.0   \n",
              "1                            (Dearbhla Hull, Zambon SpA)      749.0   \n",
              "2      (Anthony Bier, Nutrasource Pharmaceutical and ...     1073.0   \n",
              "3          (Mina Pastagia, Armata Pharmaceuticals, Inc.)      958.0   \n",
              "4      (Mary C. Drinane, Dartmouth-Hitchcock Medical ...     1043.0   \n",
              "...                                                  ...        ...   \n",
              "96592                                                NaN        NaN   \n",
              "96593                                                NaN        NaN   \n",
              "96594                                                NaN        NaN   \n",
              "96595                                                NaN        NaN   \n",
              "96596  (Chu Q, Genzyme Corporation, Framingham, MA 01...     3595.0   \n",
              "\n",
              "            network  \n",
              "0      {(nan, nan)}  \n",
              "1                {}  \n",
              "2                {}  \n",
              "3                {}  \n",
              "4                {}  \n",
              "...             ...  \n",
              "96592           NaN  \n",
              "96593           NaN  \n",
              "96594           NaN  \n",
              "96595           NaN  \n",
              "96596            {}  \n",
              "\n",
              "[96597 rows x 7 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ad1cda-0c1e-4a3c-b04b-46b38b0958c5",
      "metadata": {
        "id": "f5ad1cda-0c1e-4a3c-b04b-46b38b0958c5"
      },
      "outputs": [],
      "source": [
        "clusetring_1=concatenated_data[concatenated_data['Community'].isin(index_community_detection)]\n",
        "clustering_2=concatenated_data[~concatenated_data['Community'].isin(index_community_detection)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ec2943-131f-418c-8230-e0492add9fd1",
      "metadata": {
        "id": "e2ec2943-131f-418c-8230-e0492add9fd1",
        "outputId": "9fd2803d-34d6-46a1-cc9b-eacc3a102052"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PI</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>Terms</th>\n",
              "      <th>Node</th>\n",
              "      <th>Community</th>\n",
              "      <th>network</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dearbhla Hull</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Zambon SpA</td>\n",
              "      <td>None</td>\n",
              "      <td>(Dearbhla Hull, Zambon SpA)</td>\n",
              "      <td>749.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Anthony Bier</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Nutrasource Pharmaceutical and Nutraceutical S...</td>\n",
              "      <td>None</td>\n",
              "      <td>(Anthony Bier, Nutrasource Pharmaceutical and ...</td>\n",
              "      <td>1073.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mina Pastagia</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Armata Pharmaceuticals, Inc.</td>\n",
              "      <td>None</td>\n",
              "      <td>(Mina Pastagia, Armata Pharmaceuticals, Inc.)</td>\n",
              "      <td>958.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mary C. Drinane</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Dartmouth-Hitchcock Medical Center</td>\n",
              "      <td>None</td>\n",
              "      <td>(Mary C. Drinane, Dartmouth-Hitchcock Medical ...</td>\n",
              "      <td>1043.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>H. Frank Farmer</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Covance CRU, Inc.</td>\n",
              "      <td>None</td>\n",
              "      <td>(H. Frank Farmer, Covance CRU, Inc.)</td>\n",
              "      <td>254.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96592</th>\n",
              "      <td>Eiberg, H., Schmiegelow, K., Tsui, L.-C., Buch...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96593</th>\n",
              "      <td>Eiberg, H., Mohr, J., Nielsen, L. S.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96594</th>\n",
              "      <td>Danks, D. M., Allan, J., Anderson, C. M.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96595</th>\n",
              "      <td>Cutting, G. R.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96596</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Chu Q, Genzyme Corporation, Framingham, MA 01...</td>\n",
              "      <td>3595.0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>95772 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      PI Abstract  \\\n",
              "1                                          Dearbhla Hull      NaN   \n",
              "2                                           Anthony Bier      NaN   \n",
              "3                                          Mina Pastagia      NaN   \n",
              "4                                        Mary C. Drinane      NaN   \n",
              "7                                        H. Frank Farmer      NaN   \n",
              "...                                                  ...      ...   \n",
              "96592  Eiberg, H., Schmiegelow, K., Tsui, L.-C., Buch...      NaN   \n",
              "96593               Eiberg, H., Mohr, J., Nielsen, L. S.      NaN   \n",
              "96594           Danks, D. M., Allan, J., Anderson, C. M.      NaN   \n",
              "96595                                     Cutting, G. R.      NaN   \n",
              "96596                                                NaN      NaN   \n",
              "\n",
              "                                             affiliation Terms  \\\n",
              "1                                             Zambon SpA  None   \n",
              "2      Nutrasource Pharmaceutical and Nutraceutical S...  None   \n",
              "3                           Armata Pharmaceuticals, Inc.  None   \n",
              "4                     Dartmouth-Hitchcock Medical Center  None   \n",
              "7                                      Covance CRU, Inc.  None   \n",
              "...                                                  ...   ...   \n",
              "96592                                               None   NaN   \n",
              "96593                                               None   NaN   \n",
              "96594                                               None   NaN   \n",
              "96595                                               None   NaN   \n",
              "96596                                                NaN   NaN   \n",
              "\n",
              "                                                    Node  Community network  \n",
              "1                            (Dearbhla Hull, Zambon SpA)      749.0      {}  \n",
              "2      (Anthony Bier, Nutrasource Pharmaceutical and ...     1073.0      {}  \n",
              "3          (Mina Pastagia, Armata Pharmaceuticals, Inc.)      958.0      {}  \n",
              "4      (Mary C. Drinane, Dartmouth-Hitchcock Medical ...     1043.0      {}  \n",
              "7                   (H. Frank Farmer, Covance CRU, Inc.)      254.0      {}  \n",
              "...                                                  ...        ...     ...  \n",
              "96592                                                NaN        NaN     NaN  \n",
              "96593                                                NaN        NaN     NaN  \n",
              "96594                                                NaN        NaN     NaN  \n",
              "96595                                                NaN        NaN     NaN  \n",
              "96596  (Chu Q, Genzyme Corporation, Framingham, MA 01...     3595.0      {}  \n",
              "\n",
              "[95772 rows x 7 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clustering_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c44b66-74c1-4acd-8c30-b60fd25d3d48",
      "metadata": {
        "id": "21c44b66-74c1-4acd-8c30-b60fd25d3d48"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def name_clusters(df, text_column, cluster_column, num_clusters=5):\n",
        "    # Extract abstracts from the specified column and handle NaN values\n",
        "    abstracts = df[text_column].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(abstracts)\n",
        "\n",
        "    # KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "    # Assign cluster labels to the DataFrame\n",
        "    df[cluster_column] = clusters\n",
        "\n",
        "    return df\n",
        "\n",
        "# Assuming you have a DataFrame `clustering_2`\n",
        "clustering_2_ = name_clusters(clustering_2, 'Abstract', 'Community', num_clusters=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65cc0133-36ab-47d5-84c8-357c5bec2fbe",
      "metadata": {
        "id": "65cc0133-36ab-47d5-84c8-357c5bec2fbe",
        "outputId": "ca7bbd68-9d24-455a-b732-34ab69a7796c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\valinejadj2\\AppData\\Local\\Temp\\ipykernel_25404\\1852488632.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[cluster_column] = clusters\n",
            "C:\\Users\\valinejadj2\\AppData\\Local\\Temp\\ipykernel_25404\\1852488632.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  clustering_2_['Community']=clustering_2_.apply(lambda x: x['Community']+100000000000, axis=1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import MiniBatchKMeans  # Use MiniBatchKMeans for faster clustering\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD  # Dimensionality reduction to speed up vectorization\n",
        "\n",
        "def name_clusters(df, text_column, cluster_column, num_clusters=5, n_components=100):\n",
        "    # Extract abstracts from the specified column and handle NaN values\n",
        "    abstracts = df[text_column].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "    # TF-IDF vectorization with ngram_range (optimize based on your dataset)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))  # Bi-grams may help for better context\n",
        "    tfidf_matrix = vectorizer.fit_transform(abstracts)\n",
        "\n",
        "    # Optionally reduce dimensionality to speed up clustering\n",
        "    if n_components:\n",
        "        svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "        tfidf_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # MiniBatchKMeans clustering (faster than KMeans)\n",
        "    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=42, n_init=10, batch_size=1000)\n",
        "    clusters = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "    # Assign cluster labels to the DataFrame\n",
        "    df[cluster_column] = clusters\n",
        "\n",
        "    return df\n",
        "\n",
        "# Assuming you have a DataFrame `clustering_2`\n",
        "clustering_2_ = name_clusters(clustering_2, 'Abstract', 'Community', num_clusters=300, n_components=100)\n",
        "#clustering_2_.loc[:, 'Community'] = clustering_2_['Community'] + 100000000000\n",
        "clustering_2_['Community']=clustering_2_.apply(lambda x: x['Community']+100000000000, axis=1)\n",
        "clustering_=pd.concat([clusetring_1,clustering_2_])\n",
        "community_mapping = {val: idx + 1 for idx, val in enumerate(clustering_['Community'].unique())}\n",
        "# Map the original 'Community' values to the new values\n",
        "clustering_['Community'] = clustering_['Community'].map(community_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5870e64e-cad6-4ca7-9d56-5f79698f9208",
      "metadata": {
        "id": "5870e64e-cad6-4ca7-9d56-5f79698f9208"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d8cb50-5949-4695-98bc-f9d1b1439020",
      "metadata": {
        "id": "98d8cb50-5949-4695-98bc-f9d1b1439020",
        "outputId": "cc14e3c0-b90a-4f4f-db36-36b7d2bc7565"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PI</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>Terms</th>\n",
              "      <th>Node</th>\n",
              "      <th>Community</th>\n",
              "      <th>network</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>(nan, nan)</td>\n",
              "      <td>1</td>\n",
              "      <td>{(nan, nan)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Nicolas Roche</td>\n",
              "      <td>NaN</td>\n",
              "      <td>APHP- Hopital Cochin</td>\n",
              "      <td>None</td>\n",
              "      <td>(Nicolas Roche, APHP- Hopital Cochin)</td>\n",
              "      <td>2</td>\n",
              "      <td>{(Thierry Perez, CHRU de Lille / Hpital Calme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Thierry Perez</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CHRU de Lille / Hpital Calmette - France</td>\n",
              "      <td>None</td>\n",
              "      <td>(Thierry Perez, CHRU de Lille / Hpital Calmet...</td>\n",
              "      <td>2</td>\n",
              "      <td>{(Nicolas Roche, APHP- Hopital Cochin)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Remi Rabasa-Lhoret</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Institut de Recherches Cliniques de Montreal</td>\n",
              "      <td>None</td>\n",
              "      <td>(Remi Rabasa-Lhoret, Institut de Recherches Cl...</td>\n",
              "      <td>3</td>\n",
              "      <td>{(Yves Berthiaume, Institut de Recherches Clin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Yves Berthiaume</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Institut de Recherches Cliniques de Montreal</td>\n",
              "      <td>None</td>\n",
              "      <td>(Yves Berthiaume, Institut de Recherches Clini...</td>\n",
              "      <td>3</td>\n",
              "      <td>{(Remi Rabasa-Lhoret, Institut de Recherches C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96592</th>\n",
              "      <td>Eiberg, H., Schmiegelow, K., Tsui, L.-C., Buch...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>276</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96593</th>\n",
              "      <td>Eiberg, H., Mohr, J., Nielsen, L. S.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>276</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96594</th>\n",
              "      <td>Danks, D. M., Allan, J., Anderson, C. M.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>276</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96595</th>\n",
              "      <td>Cutting, G. R.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>276</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96596</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Chu Q, Genzyme Corporation, Framingham, MA 01...</td>\n",
              "      <td>276</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>96597 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      PI Abstract  \\\n",
              "0                                                    NaN      NaN   \n",
              "5                                          Nicolas Roche      NaN   \n",
              "6                                          Thierry Perez      NaN   \n",
              "19                                    Remi Rabasa-Lhoret      NaN   \n",
              "20                                       Yves Berthiaume      NaN   \n",
              "...                                                  ...      ...   \n",
              "96592  Eiberg, H., Schmiegelow, K., Tsui, L.-C., Buch...      NaN   \n",
              "96593               Eiberg, H., Mohr, J., Nielsen, L. S.      NaN   \n",
              "96594           Danks, D. M., Allan, J., Anderson, C. M.      NaN   \n",
              "96595                                     Cutting, G. R.      NaN   \n",
              "96596                                                NaN      NaN   \n",
              "\n",
              "                                        affiliation Terms  \\\n",
              "0                                               NaN  None   \n",
              "5                              APHP- Hopital Cochin  None   \n",
              "6         CHRU de Lille / Hpital Calmette - France  None   \n",
              "19     Institut de Recherches Cliniques de Montreal  None   \n",
              "20     Institut de Recherches Cliniques de Montreal  None   \n",
              "...                                             ...   ...   \n",
              "96592                                          None   NaN   \n",
              "96593                                          None   NaN   \n",
              "96594                                          None   NaN   \n",
              "96595                                          None   NaN   \n",
              "96596                                           NaN   NaN   \n",
              "\n",
              "                                                    Node  Community  \\\n",
              "0                                             (nan, nan)          1   \n",
              "5                  (Nicolas Roche, APHP- Hopital Cochin)          2   \n",
              "6      (Thierry Perez, CHRU de Lille / Hpital Calmet...          2   \n",
              "19     (Remi Rabasa-Lhoret, Institut de Recherches Cl...          3   \n",
              "20     (Yves Berthiaume, Institut de Recherches Clini...          3   \n",
              "...                                                  ...        ...   \n",
              "96592                                                NaN        276   \n",
              "96593                                                NaN        276   \n",
              "96594                                                NaN        276   \n",
              "96595                                                NaN        276   \n",
              "96596  (Chu Q, Genzyme Corporation, Framingham, MA 01...        276   \n",
              "\n",
              "                                                 network  \n",
              "0                                           {(nan, nan)}  \n",
              "5      {(Thierry Perez, CHRU de Lille / Hpital Calme...  \n",
              "6                {(Nicolas Roche, APHP- Hopital Cochin)}  \n",
              "19     {(Yves Berthiaume, Institut de Recherches Clin...  \n",
              "20     {(Remi Rabasa-Lhoret, Institut de Recherches C...  \n",
              "...                                                  ...  \n",
              "96592                                                NaN  \n",
              "96593                                                NaN  \n",
              "96594                                                NaN  \n",
              "96595                                                NaN  \n",
              "96596                                                 {}  \n",
              "\n",
              "[96597 rows x 7 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clustering_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "405d086c-d91a-4072-9bea-4e51d22d2053",
      "metadata": {
        "scrolled": true,
        "id": "405d086c-d91a-4072-9bea-4e51d22d2053",
        "outputId": "bac88afc-e5ab-40c0-87a6-53603a5a2c4a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Community</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Terms</th>\n",
              "      <th>Size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>279</td>\n",
              "      <td>DESCRIPTION (provided by applicant):    This m...</td>\n",
              "      <td>[Lung diseases, Research Training, Lung diseas...</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>280</td>\n",
              "      <td>DESCRIPTION (provided by applicant): Molecular...</td>\n",
              "      <td>[Address, Affinity, Alzheimer's Disease, Apica...</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>281</td>\n",
              "      <td>[unreadable] DESCRIPTION (provided by applic...</td>\n",
              "      <td>[immune tolerance /unresponsiveness, model, re...</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>282</td>\n",
              "      <td>DESCRIPTION (provided by applicant): This is a...</td>\n",
              "      <td>[Childhood, Research Training, Translational R...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>283</td>\n",
              "      <td>ABSTRACT NOT PROVIDED, Abstract Not Provided.</td>\n",
              "      <td>[pulmonary diffusion, respiratory disorder che...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>283 rows  4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Community                                           Abstract  \\\n",
              "0            1                                                      \n",
              "1            2                                                      \n",
              "2            3                                                      \n",
              "3            4                                                      \n",
              "4            5                                                      \n",
              "..         ...                                                ...   \n",
              "278        279  DESCRIPTION (provided by applicant):    This m...   \n",
              "279        280  DESCRIPTION (provided by applicant): Molecular...   \n",
              "280        281    [unreadable] DESCRIPTION (provided by applic...   \n",
              "281        282  DESCRIPTION (provided by applicant): This is a...   \n",
              "282        283      ABSTRACT NOT PROVIDED, Abstract Not Provided.   \n",
              "\n",
              "                                                 Terms  Size  \n",
              "0                                                   []     1  \n",
              "1                                                   []     2  \n",
              "2                                                   []     2  \n",
              "3                                                   []     2  \n",
              "4                                                   []     2  \n",
              "..                                                 ...   ...  \n",
              "278  [Lung diseases, Research Training, Lung diseas...    27  \n",
              "279  [Address, Affinity, Alzheimer's Disease, Apica...    33  \n",
              "280  [immune tolerance /unresponsiveness, model, re...    54  \n",
              "281  [Childhood, Research Training, Translational R...     6  \n",
              "282  [pulmonary diffusion, respiratory disorder che...     2  \n",
              "\n",
              "[283 rows x 4 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def sum_strings(series):\n",
        "    # Convert all items to string, handling NaNs\n",
        "    return ', '.join(str(item) for item in series if pd.notna(item))\n",
        "\n",
        "# Function to clean and flatten terms\n",
        "def sum_terms(series):\n",
        "    result = []\n",
        "    for item in series:\n",
        "        if isinstance(item, list):  # Check if item is a list\n",
        "            for subitem in item:\n",
        "                if isinstance(subitem, str):  # Ensure it's a string\n",
        "                    subitem_cleaned = subitem.strip('[]')  # Remove surrounding brackets\n",
        "                    result.extend(subitem_cleaned.split(','))  # Split and add terms\n",
        "        elif pd.notna(item):  # Handle non-list, non-NaN values\n",
        "            item_cleaned = str(item).strip('[]')  # Convert to string and remove brackets\n",
        "            result.extend(item_cleaned.split(','))  # Split and add terms\n",
        "    # Remove extra whitespace from terms\n",
        "    return [term.strip() for term in result if term.strip()]\n",
        "\n",
        "# Group by 'Community' and aggregate columns using sum_strings and sum_terms functions\n",
        "result = clustering_.groupby('Community').agg(\n",
        "    Abstract=('Abstract', sum_strings),\n",
        "    Terms=('Terms', sum_terms),\n",
        "    Size=('Community', 'size')  # Count of occurrences\n",
        ").reset_index()\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e37395-47da-4773-af17-3c7f8bf37c90",
      "metadata": {
        "id": "72e37395-47da-4773-af17-3c7f8bf37c90",
        "outputId": "c6921d4a-10cb-4388-a0f0-50d2bf7edc15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nulls in 'c.BriefSummary': 0\n",
            "Number of empty strings in 'c.BriefSummary': 130\n"
          ]
        }
      ],
      "source": [
        "num_nulls = result['Abstract'].isnull().sum()\n",
        "print(f\"Number of nulls in 'c.BriefSummary': {num_nulls}\")\n",
        "num_empty_strings = (result['Abstract'] == '').sum()\n",
        "print(f\"Number of empty strings in 'c.BriefSummary': {num_empty_strings}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fc02de2-9e1c-44b1-923d-5d284b8cdd99",
      "metadata": {
        "id": "6fc02de2-9e1c-44b1-923d-5d284b8cdd99",
        "outputId": "cc32dc83-6254-4a95-cdd5-9235f9ff2f84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "93105"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['Size'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28f959d-7f55-4650-bfb1-dd2dc672279b",
      "metadata": {
        "id": "b28f959d-7f55-4650-bfb1-dd2dc672279b"
      },
      "outputs": [],
      "source": [
        "result.to_csv('LLM_input.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857537eb-09ab-4411-9180-17630fffe4b6",
      "metadata": {
        "id": "857537eb-09ab-4411-9180-17630fffe4b6"
      },
      "source": [
        "Run LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02a56604-2f27-4925-8aa6-33a6c3781b98",
      "metadata": {
        "id": "02a56604-2f27-4925-8aa6-33a6c3781b98",
        "outputId": "9887e066-64b7-4c9b-fed8-69cb48a61af4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.3</th>\n",
              "      <th>Unnamed: 0.2</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Community</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Terms</th>\n",
              "      <th>Size</th>\n",
              "      <th>frequent_words</th>\n",
              "      <th>Mistral</th>\n",
              "      <th>Llama_3_n</th>\n",
              "      <th>term_1n</th>\n",
              "      <th>term_2n</th>\n",
              "      <th>term_3n</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Development of a New Tool for Dyspnea Measurem...</td>\n",
              "      <td>['Cross sectional psychometric evaluation of a...</td>\n",
              "      <td>2</td>\n",
              "      <td>Cross sectional psychometric evaluation of a s...</td>\n",
              "      <td>1. Psychometrics: The study focused on the psy...</td>\n",
              "      <td>1. Psychometrics\\n2. Questionnaire Design\\n3. ...</td>\n",
              "      <td>Psychometrics</td>\n",
              "      <td>Questionnaire Design</td>\n",
              "      <td>Dyspnea Measurement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>The Effects of a Low Glycemic Load Diet on Dys...</td>\n",
              "      <td>['Low Glycemic Load Diet', 'Low Glycemic Load ...</td>\n",
              "      <td>2</td>\n",
              "      <td>Low Glycemic Load Diet</td>\n",
              "      <td>1. Glycemic Control\\n2. Nutrition Management\\n...</td>\n",
              "      <td>1. Glycemic Index\\n2. Glycemic Load\\n3. Nutrit...</td>\n",
              "      <td>Glycemic Index</td>\n",
              "      <td>Glycemic Load</td>\n",
              "      <td>Nutritional Assessment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>A Phase 1/2 Study to Evaluate the Safety, Tole...</td>\n",
              "      <td>['PTI-808, Placebo, PTI-428, PTI-801', 'PTI-80...</td>\n",
              "      <td>2</td>\n",
              "      <td>PTI-808, Placebo, PTI-428, PTI-801</td>\n",
              "      <td>1. Pharmacokinetics: The study will evaluate t...</td>\n",
              "      <td>1. Pharmacokinetics\\n2. Pharmacodynamics\\n3. D...</td>\n",
              "      <td>Pharmacokinetics</td>\n",
              "      <td>Pharmacodynamics</td>\n",
              "      <td>Drug-drug interactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>In Vitro Comparison of Continuous and Breath-s...</td>\n",
              "      <td>['Nebulization of Amikacin during NIV (RR: 25 ...</td>\n",
              "      <td>7</td>\n",
              "      <td>Nebulization of Amikacin during NIV (RR: 25 cy...</td>\n",
              "      <td>1. Nebulization\\n2. Inhaled and Lost Doses\\n3....</td>\n",
              "      <td>Keywords: Nebulization, Amikacin, Non-invasive...</td>\n",
              "      <td>Nebulization</td>\n",
              "      <td>Amikacin</td>\n",
              "      <td>invasive ventilation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>Intraduodenal Aspiration Study to Assess the B...</td>\n",
              "      <td>['Pancrelipase', 'Pancrelipase']</td>\n",
              "      <td>2</td>\n",
              "      <td>Pancrelipase</td>\n",
              "      <td>1. Pancreatic Enzyme Replacement Therapy (PERT...</td>\n",
              "      <td>1. Pancrelipase\\n2. Pancreatic Insufficiency\\n...</td>\n",
              "      <td>Pancrelipase</td>\n",
              "      <td>Pancreatic Insufficiency</td>\n",
              "      <td>Bioavailability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>108</td>\n",
              "      <td>108</td>\n",
              "      <td>108</td>\n",
              "      <td>108</td>\n",
              "      <td>109</td>\n",
              "      <td>Project Summary/Abstract Cilia dysfunction lea...</td>\n",
              "      <td>['[Actins,  Adaptor Signaling Protein,  Addres...</td>\n",
              "      <td>4</td>\n",
              "      <td>[Actins,  Adaptor Signaling Protein,  Address,...</td>\n",
              "      <td>1. Microtubule dynamics\\n2. Protein traffickin...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>109</td>\n",
              "      <td>109</td>\n",
              "      <td>109</td>\n",
              "      <td>109</td>\n",
              "      <td>110</td>\n",
              "      <td>...</td>\n",
              "      <td>['[bacteria infection mechanism, respiratory i...</td>\n",
              "      <td>7</td>\n",
              "      <td>[16S ribosomal RNA sequencing,  Adopted,  Adop...</td>\n",
              "      <td>1. Bioinformatics - The use of computational t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>110</td>\n",
              "      <td>110</td>\n",
              "      <td>110</td>\n",
              "      <td>110</td>\n",
              "      <td>111</td>\n",
              "      <td>DESCRIPTION (provided by applicant): Cystic Fi...</td>\n",
              "      <td>['[absorption, Affect, Animal Model, Area, Bac...</td>\n",
              "      <td>11</td>\n",
              "      <td>[absorption, Affect, Animal Model, Area, Bacte...</td>\n",
              "      <td>1. Comparative Pathology: The study of the dif...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>111</td>\n",
              "      <td>111</td>\n",
              "      <td>111</td>\n",
              "      <td>111</td>\n",
              "      <td>112</td>\n",
              "      <td>This project will test the hypothesis that som...</td>\n",
              "      <td>['[lung disorder, chloride channels, protein d...</td>\n",
              "      <td>3</td>\n",
              "      <td>[lung disorder, chloride channels, protein deg...</td>\n",
              "      <td>1. Molecular biology: Genetic engineering, gen...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>112</td>\n",
              "      <td>112</td>\n",
              "      <td>112</td>\n",
              "      <td>112</td>\n",
              "      <td>113</td>\n",
              "      <td>The overall objective of this project is to de...</td>\n",
              "      <td>['[protein protein interaction, phosphoprotein...</td>\n",
              "      <td>4</td>\n",
              "      <td>[protein protein interaction, phosphoproteins,...</td>\n",
              "      <td>1. Bioinformatics\\n2. Protein structure and fu...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>113 rows  14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  Community  \\\n",
              "0               0             0             0           0          1   \n",
              "1               1             1             1           1          2   \n",
              "2               2             2             2           2          3   \n",
              "3               3             3             3           3          4   \n",
              "4               4             4             4           4          5   \n",
              "..            ...           ...           ...         ...        ...   \n",
              "108           108           108           108         108        109   \n",
              "109           109           109           109         109        110   \n",
              "110           110           110           110         110        111   \n",
              "111           111           111           111         111        112   \n",
              "112           112           112           112         112        113   \n",
              "\n",
              "                                              Abstract  \\\n",
              "0    Development of a New Tool for Dyspnea Measurem...   \n",
              "1    The Effects of a Low Glycemic Load Diet on Dys...   \n",
              "2    A Phase 1/2 Study to Evaluate the Safety, Tole...   \n",
              "3    In Vitro Comparison of Continuous and Breath-s...   \n",
              "4    Intraduodenal Aspiration Study to Assess the B...   \n",
              "..                                                 ...   \n",
              "108  Project Summary/Abstract Cilia dysfunction lea...   \n",
              "109                                                ...   \n",
              "110  DESCRIPTION (provided by applicant): Cystic Fi...   \n",
              "111  This project will test the hypothesis that som...   \n",
              "112  The overall objective of this project is to de...   \n",
              "\n",
              "                                                 Terms  Size  \\\n",
              "0    ['Cross sectional psychometric evaluation of a...     2   \n",
              "1    ['Low Glycemic Load Diet', 'Low Glycemic Load ...     2   \n",
              "2    ['PTI-808, Placebo, PTI-428, PTI-801', 'PTI-80...     2   \n",
              "3    ['Nebulization of Amikacin during NIV (RR: 25 ...     7   \n",
              "4                     ['Pancrelipase', 'Pancrelipase']     2   \n",
              "..                                                 ...   ...   \n",
              "108  ['[Actins,  Adaptor Signaling Protein,  Addres...     4   \n",
              "109  ['[bacteria infection mechanism, respiratory i...     7   \n",
              "110  ['[absorption, Affect, Animal Model, Area, Bac...    11   \n",
              "111  ['[lung disorder, chloride channels, protein d...     3   \n",
              "112  ['[protein protein interaction, phosphoprotein...     4   \n",
              "\n",
              "                                        frequent_words  \\\n",
              "0    Cross sectional psychometric evaluation of a s...   \n",
              "1                               Low Glycemic Load Diet   \n",
              "2                   PTI-808, Placebo, PTI-428, PTI-801   \n",
              "3    Nebulization of Amikacin during NIV (RR: 25 cy...   \n",
              "4                                         Pancrelipase   \n",
              "..                                                 ...   \n",
              "108  [Actins,  Adaptor Signaling Protein,  Address,...   \n",
              "109  [16S ribosomal RNA sequencing,  Adopted,  Adop...   \n",
              "110  [absorption, Affect, Animal Model, Area, Bacte...   \n",
              "111  [lung disorder, chloride channels, protein deg...   \n",
              "112  [protein protein interaction, phosphoproteins,...   \n",
              "\n",
              "                                               Mistral  \\\n",
              "0    1. Psychometrics: The study focused on the psy...   \n",
              "1    1. Glycemic Control\\n2. Nutrition Management\\n...   \n",
              "2    1. Pharmacokinetics: The study will evaluate t...   \n",
              "3    1. Nebulization\\n2. Inhaled and Lost Doses\\n3....   \n",
              "4    1. Pancreatic Enzyme Replacement Therapy (PERT...   \n",
              "..                                                 ...   \n",
              "108  1. Microtubule dynamics\\n2. Protein traffickin...   \n",
              "109  1. Bioinformatics - The use of computational t...   \n",
              "110  1. Comparative Pathology: The study of the dif...   \n",
              "111  1. Molecular biology: Genetic engineering, gen...   \n",
              "112  1. Bioinformatics\\n2. Protein structure and fu...   \n",
              "\n",
              "                                             Llama_3_n           term_1n  \\\n",
              "0    1. Psychometrics\\n2. Questionnaire Design\\n3. ...     Psychometrics   \n",
              "1    1. Glycemic Index\\n2. Glycemic Load\\n3. Nutrit...    Glycemic Index   \n",
              "2    1. Pharmacokinetics\\n2. Pharmacodynamics\\n3. D...  Pharmacokinetics   \n",
              "3    Keywords: Nebulization, Amikacin, Non-invasive...      Nebulization   \n",
              "4    1. Pancrelipase\\n2. Pancreatic Insufficiency\\n...      Pancrelipase   \n",
              "..                                                 ...               ...   \n",
              "108                                                NaN               NaN   \n",
              "109                                                NaN               NaN   \n",
              "110                                                NaN               NaN   \n",
              "111                                                NaN               NaN   \n",
              "112                                                NaN               NaN   \n",
              "\n",
              "                      term_2n                 term_3n  \n",
              "0        Questionnaire Design     Dyspnea Measurement  \n",
              "1               Glycemic Load  Nutritional Assessment  \n",
              "2            Pharmacodynamics  Drug-drug interactions  \n",
              "3                    Amikacin    invasive ventilation  \n",
              "4    Pancreatic Insufficiency         Bioavailability  \n",
              "..                        ...                     ...  \n",
              "108                       NaN                     NaN  \n",
              "109                       NaN                     NaN  \n",
              "110                       NaN                     NaN  \n",
              "111                       NaN                     NaN  \n",
              "112                       NaN                     NaN  \n",
              "\n",
              "[113 rows x 14 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "result = pd.read_csv('C:/Users/Desktop/data.csv')\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5acb81c-77a8-4a34-90a5-8a41148eb3cf",
      "metadata": {
        "id": "d5acb81c-77a8-4a34-90a5-8a41148eb3cf"
      },
      "outputs": [],
      "source": [
        "def create_Cluster_node(properties):\n",
        "    with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "        with driver.session() as session:\n",
        "            query = (\n",
        "                \"CREATE (Cl:Cluster {\"\n",
        "                'Cluster_ID: $Cluster_ID,'\n",
        "                'Cluster_Size: $Cluster_Size,'\n",
        "                'Evidence: $Evidence,'\n",
        "                'Key_trems: $Key_trems'\n",
        "                \"})\"\n",
        "            )\n",
        "            session.run(query, **properties)\n",
        "\n",
        "for i in result.index:\n",
        "    event_properties = {\n",
        "        'Cluster_ID': str(result['Community'][i]),\n",
        "        'Cluster_Size': str(result['Size'][i]),\n",
        "        'Evidence': str(result['Abstract'][i]),\n",
        "        'Key_trems': str(result['frequent_words'][i]),\n",
        "    }\n",
        "    create_Cluster_node(event_properties)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0d5a2d-ba47-46c8-9d97-f48faa2f8f4a",
      "metadata": {
        "id": "3b0d5a2d-ba47-46c8-9d97-f48faa2f8f4a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "event_properties = {'GARDID': 'GARD0006233' ,'GARDname': 'Cystic fibrosis'}\n",
        "create_Gard_node(event_properties)\n",
        "'''\n",
        "\n",
        "def create_relationship_between_nodes(node1_label, node2_label, relationship_type, GARDname_):\n",
        "   with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "    with driver.session() as session:\n",
        "        query = (\n",
        "            f\"MATCH (n1:{node1_label}), (n2:{node2_label}) \"\n",
        "            f\"WHERE n2.GARDname = '{GARDname_}' \"  # Enclose GARDname_ in single quotes\n",
        "            f\"CREATE (n1)-[:{relationship_type}]->(n2)\"\n",
        "        )\n",
        "        session.run(query)\n",
        "\n",
        "# Example usage:\n",
        "create_relationship_between_nodes('Cluster', 'Gard', \"Cluster_to_GARD\", 'Cystic fibrosis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ce123a1-ba30-4828-9b3e-bea63fac8ce7",
      "metadata": {
        "id": "4ce123a1-ba30-4828-9b3e-bea63fac8ce7"
      },
      "outputs": [],
      "source": [
        "def create_relationship_cluster_researcher(community, name, affiliation):\n",
        "  with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "    with driver.session() as session:\n",
        "        query = (\n",
        "            \"MATCH (n1:Cluster), (n2:Researcher) \"\n",
        "            \"WHERE n1.Cluster_ID = $community AND n2.Name = $name AND n2.Aff_name = $affiliation \"\n",
        "            \"CREATE (n1)-[:Related_cluster]->(n2)\"\n",
        "        )\n",
        "        session.run(query, community=community, name=name, affiliation=affiliation)\n",
        "\n",
        "for i in clustering_.index:\n",
        "    node_value = clustering_['Node'][i]\n",
        "    # Check if node_value is a tuple/list, otherwise skip or handle it differently\n",
        "    if isinstance(node_value, (tuple, list)) and len(node_value) >= 2:\n",
        "        create_relationship_cluster_researcher(\n",
        "            str(clustering_['Community'][i]),\n",
        "            node_value[0],  # Researcher name\n",
        "            node_value[1]   # Affiliation\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d365b8a-4a79-41c8-9e78-82910fa17940",
      "metadata": {
        "id": "7d365b8a-4a79-41c8-9e78-82910fa17940",
        "outputId": "23d18b04-a53d-4cad-a24c-7428e3245bd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(result['term_3n'][60]) != str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b5d8fd-bcd0-484f-b508-047b3d2a3642",
      "metadata": {
        "id": "f9b5d8fd-bcd0-484f-b508-047b3d2a3642"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "def extract_terms(sentence):\n",
        "    # Check if the sentence is a valid string\n",
        "    if not isinstance(sentence, str):return []  # Return empty list if not a string\n",
        "    # Regular expression to match the terms\n",
        "    pattern = r'\\d+\\.\\s*(.*?)(?:\\n|<\\|end_of_text\\|>|\\s*$)'\n",
        "    # Find all matches in the sentence\n",
        "    terms = re.findall(pattern, sentence)\n",
        "    # Return only the first four terms, or all if less than four\n",
        "    return terms[:4]\n",
        "\n",
        "def extract_and_create_expertise_nodes():\n",
        "  with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "    with driver.session() as session:\n",
        "        # Query to retrieve all Cluster nodes with Evidence\n",
        "        query = \"MATCH (c:Cluster) RETURN c.Cluster_ID AS cluster_id, c.Evidence AS evidence\"\n",
        "        results = session.run(query)\n",
        "\n",
        "        for record in results:\n",
        "          cluster_id = record['cluster_id']\n",
        "          if  cluster_id:\n",
        "            try:\n",
        "              evidence = result['Llama_3_n'][result['Community'].astype(str) == str(cluster_id)].iloc[0] #record['evidence']\n",
        "            except: evidence=None\n",
        "              # Check if evidence is not None\n",
        "            if evidence:\n",
        "                # Split evidence into sentences\n",
        "                sentences =  [result['term_1n'],result['term_2n'],result['term_3n']]  #extract_terms(evidence)\n",
        "\n",
        "                for sentence in sentences:\n",
        "                   if type(sentence) == str and sentence != 'None':\n",
        "                    # Create a new Expertise node for each sentence\n",
        "                    create_expertise_node(session, sentence, cluster_id)\n",
        "\n",
        "def create_expertise_node(session, sentence, cluster_id):\n",
        "    # Create a new Expertise node with a relationship to the Cluster\n",
        "    query = (\n",
        "        \"CREATE (e:Expertise {Summarized_expertise: $sentence}) \"\n",
        "        \"WITH e \"\n",
        "        \"MATCH (c:Cluster {Cluster_ID: $cluster_id}) \"\n",
        "        \"CREATE (c)-[:HAS_EXPERTISE]->(e)\"\n",
        "    )\n",
        "    session.run(query, sentence=sentence, cluster_id=cluster_id)\n",
        "\n",
        "# Call the function\n",
        "extract_and_create_expertise_nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a1fefe-3097-4304-ba75-56e2bf9fcaaa",
      "metadata": {
        "id": "03a1fefe-3097-4304-ba75-56e2bf9fcaaa"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "def extract_terms(sentence):\n",
        "    # Check if the sentence is a valid string\n",
        "    if not isinstance(sentence, str):return []  # Return empty list if not a string\n",
        "    # Regular expression to match the terms\n",
        "    pattern = r'\\d+\\.\\s*(.*?)(?:\\n|<\\|end_of_text\\|>|\\s*$)'\n",
        "    # Find all matches in the sentence\n",
        "    terms = re.findall(pattern, sentence)\n",
        "    # Return only the first four terms, or all if less than four\n",
        "    return terms[:4]\n",
        "\n",
        "def extract_and_create_expertise_nodes():\n",
        "  with GraphDatabase.driver(uri, auth=(username, password)) as driver:\n",
        "    with driver.session() as session:\n",
        "        # Query to retrieve all Cluster nodes with Evidence\n",
        "        query = \"MATCH (c:Cluster) RETURN c.Cluster_ID AS cluster_id, c.Evidence AS evidence\"\n",
        "        results = session.run(query)\n",
        "\n",
        "        for i in result.index:\n",
        "                cluster_id = str(result['Community'][i])\n",
        "                sentences =  [result['term_1n'][i],result['term_2n'][i],result['term_3n'][i]]  #extract_terms(evidence)\n",
        "                for sentence in sentences:\n",
        "                   if type(sentence) == str and sentence != 'None':\n",
        "                    # Create a new Expertise node for each sentence\n",
        "                    create_expertise_node(session, sentence, cluster_id)\n",
        "\n",
        "def create_expertise_node(session, sentence, cluster_id):\n",
        "    # Create a new Expertise node with a relationship to the Cluster\n",
        "    query = (\n",
        "        \"CREATE (e:Expertise {Summarized_expertise: $sentence}) \"\n",
        "\n",
        "    )\n",
        "    session.run(query, sentence=sentence, cluster_id=cluster_id)\n",
        "\n",
        "# Call the function\n",
        "extract_and_create_expertise_nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93fe6cf6-342e-45f7-af07-4a76f28d1d86",
      "metadata": {
        "id": "93fe6cf6-342e-45f7-af07-4a76f28d1d86"
      },
      "outputs": [],
      "source": [
        "def export_data():\n",
        "    with driver.session() as session:\n",
        "        # Adjust the path and format as needed (CSV, JSON, etc.)\n",
        "        export_query = \"\"\"\n",
        "        CALL apoc.export.csv.all(\"file:///exported_data.csv\", {})\n",
        "        \"\"\"\n",
        "        session.run(export_query)\n",
        "        print(\"Data export completed.\")\n",
        "\n",
        "# Run the export\n",
        "export_data()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}