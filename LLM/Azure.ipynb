{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS1ydSYuhB-f",
        "outputId": "c6e94614-7613-4fb6-c13c-0c3f44a3c83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "\n",
        "base_url = os.getenv(\"https://DeepSeek-R1-qdvrd.eastus2.models.ai.azure.com\")\n",
        "api_key=os.getenv(\"api_key\")\n",
        "\n",
        "print(base_url)\n",
        "print(api_key)\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url =\"https://Meta-Llama-3-70B-Instruct-vycmo-serverless.eastus2.inference.ai.azure.com\",\n",
        "  api_key= \"api_key\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "# llama auto annotationls\n",
        "#####################\n",
        "\n",
        "def runllama(row, max_retries=10):\n",
        "    ###############################################\n",
        "    #Fixed Statistics ... descriptions out of order\n",
        "    ###############################################\n",
        "\n",
        "    elig = row['a.EligibilityCriteria'].replace('\\n','')\n",
        "    print(elig)\n",
        "\n",
        "\n",
        "\n",
        "    prompt = \"\"\"Read the eligibility criteria from a clinical trial below and return a short summary that encapsulates the important details, including labratory values. Ensure the summary is concise and captures the essence of the clinical trial.\n",
        "\n",
        "    Eligibility Criteria: \"\"\" + elig + \"\"\"\\n\n",
        "\n",
        "    From the summary, generate any possible specific categories for the data, for example generate the fields:\n",
        "    Age\n",
        "    Sex\n",
        "    Ethnicity\n",
        "    Family Medical History\n",
        "    Diagnosis History\n",
        "    Medication History\n",
        "    Vital Sign Values\n",
        "    Laboratory Values\n",
        "    Diseases\n",
        "\n",
        "    And any additional categories you can think of\n",
        "    \"\"\"\n",
        "\n",
        "    '''\n",
        "    prompt = \"\"\"Read the eligibility criteria from a clinical trial below and return a short summary that encapsulates the important details, including labratory values. Ensure the summary is concise and captures the essence of the clinical trial.\n",
        "\n",
        "    Eligibility Criteria: \"\"\" + elig + \"\"\"\\n\n",
        "\n",
        "    From the summary, extract the inclusion criteria and exclusion criteria formatted in JSON format, ONLY return the JSON output.\n",
        "\n",
        "    In the JSON, for both the inclusion and exclusion criteria sections only add the following fields:\n",
        "    Age: Exclude the units like \"years\". Example: \"['> 18']\"\n",
        "    Sex: Standardize to the values of \"male\" or \"female\". Example: convert 'm' or 'boy' to \"['male']\"\n",
        "    Ethnicity:\n",
        "    Family Medical History:\n",
        "    Diagnosis History:\n",
        "    Medication History:\n",
        "    Vital Sign Values: Example {{'Height': '6 ft', 'Weight': '160 lbs'}}\n",
        "    Laboratory Values:\n",
        "    Diseases:\n",
        "    Other Statuses:\n",
        "\n",
        "    All of these will be lists of values besides the Laboratory Values and Vital Sign Values which will be a dictionary, if there is nothing available for the section then just place an empty list\n",
        "\n",
        "    Remember to only return the JSON output, nothing else\n",
        "    \"\"\"\n",
        "    '''\n",
        "\n",
        "    message_text = [{\"role\":\"system\",\"content\":\"You are a medical researcher extracting knowledge assertions from scientific literature.\"},\n",
        "                    {\"role\":\"user\",\"content\":prompt}]\n",
        "\n",
        "    retry_count = 0\n",
        "    base_sleep = 2  # Base sleep time in seconds\n",
        "    max_tokens = 7000\n",
        "    while retry_count < max_retries:\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                messages=message_text,\n",
        "                model=\"azureai\",\n",
        "                max_tokens = max_tokens,\n",
        "                temperature=0.8,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0,\n",
        "                stop=None,\n",
        "                seed=42,\n",
        "                extra_body={\n",
        "                  \"use_beam_search\": True,\n",
        "                  \"best_of\": 4\n",
        "                }\n",
        "            )\n",
        "            out = completion.choices[0].message.content\n",
        "            print(out)\n",
        "            return out\n",
        "\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):  # Check if the error is due to rate limiting\n",
        "                wait = base_sleep * (2 ** retry_count) + random.uniform(0, 1)\n",
        "                print(f\"Rate limit exceeded. Waiting {wait:.2f} seconds before retrying...\")\n",
        "                time.sleep(wait)\n",
        "            elif \"400\" in str(e):\n",
        "                max_tokens -= 100\n",
        "                print(f\"Max Tokens: {max_tokens}\")\n",
        "            else:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "            retry_count += 1\n",
        "\n",
        "    return None\n",
        "\n",
        "#####################\n",
        "# Run auto annotations\n",
        "#####################\n",
        "\n",
        "#save_path = '/home/leadmandj/RDAS/RDAS_CTKG/eligibility_extraction/eligibility_llama3_extracted.csv'\n",
        "save_path = 'topics_output.csv'\n",
        "df = pd.read_csv('/content/topics.csv', index_col=False)\n",
        "df = df[:10] #TEST\n",
        "\n",
        "\n",
        "\n",
        "r,c = df.shape\n",
        "llama_output = list()\n",
        "\n",
        "\n",
        "for idx in range(r):\n",
        "    row = df.iloc[idx]\n",
        "    print(row)\n",
        "    temp = runllama(row)\n",
        "    print(temp)\n",
        "    llama_output.append([row['m.NCTId'], row['a.EligibilityCriteria'], temp])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(llama_output)\n",
        "df.to_csv(save_path, index=False)"
      ],
      "metadata": {
        "id": "2uKCAxMkhbq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Shailender-Youtube/DeepSeek-Azure-AI-Foundry-WebApp"
      ],
      "metadata": {
        "id": "g-P0TRIoQpEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-core\n",
        "!pip install azure-ai-inference\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "yvoT3CULQlBf",
        "outputId": "02fad778-d9b7-4c4c-9e19-a0342691d0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure-core in /usr/local/lib/python3.11/dist-packages (1.32.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-core) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core) (2025.1.31)\n",
            "Collecting azure-ai-inference\n",
            "  Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-inference) (0.7.2)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-inference) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-inference) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-ai-inference) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-ai-inference) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2025.1.31)\n",
            "Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: azure-ai-inference\n",
            "Successfully installed azure-ai-inference-1.0.0b9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "azure"
                ]
              },
              "id": "052025b621644cf3a97b8235e6956dc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "AZURE_ENDPOINT = \"https://DeepSeek-R1-qdvrd.eastus2.models.ai.azure.com\" #os.getenv(\"AZURE_ENDPOINT\")\n",
        "AZURE_KEY =\"api_key\"# os.getenv(\"AZURE_KEY\")\n",
        "\n",
        "client = ChatCompletionsClient(\n",
        "    endpoint=AZURE_ENDPOINT, #os.environ[\"AZURE_ENDPOINT\"],\n",
        "    credential=AzureKeyCredential(AZURE_KEY),#os.environ[\"AZURE_KEY\"]),\n",
        ")\n",
        "\n",
        "response = client.complete(\n",
        "    messages=[\n",
        "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "        UserMessage(content=\"Can you use python for creating a frontend app?\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Response:\", response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ldQEmWcQiqd",
        "outputId": "c4aabc24-34e9-48fc-dd39-c230daed8a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: <think>\n",
            "Okay, the user is asking if Python can be used to create a frontend app. Let me think.\n",
            "\n",
            "First, I know Python is mainly a backend language, but there are some frameworks that let you build frontends. I should mention those. Like Tkinter for desktop GUIs. Maybe Kivy for mobile apps? Oh, and there's also frameworks that work with web frontends, like Django or Flask, but those are more backend. Wait, but for web frontends, Python isn't typically used. JavaScript is standard there. However, there are tools like Brython or Skulpt that transpile Python to JavaScript. Also, newer tools like NiceGUI or Streamlit for dashboards. Anvil is another one I've heard of for full-stack apps in Python.\n",
            "\n",
            "I should outline the different scenarios: desktop, mobile, web. For each, list the Python frameworks. Also note that while Python can be used for frontend, it's not as common as JavaScript, especially for complex web UIs. Mention the pros and cons. Maybe give a simple example with Tkinter and another with Streamlit to show how it's done. Remind the user that for web, JavaScript is still dominant, but Python options exist for simpler cases or specific needs.\n",
            "</think>\n",
            "\n",
            "Yes, Python can be used to create frontend applications, though it is not as common as JavaScript for traditional web frontends. Here are the main approaches and tools for building frontend-like apps with Python:\n",
            "\n",
            "---\n",
            "\n",
            "### **1. Desktop GUI Applications**\n",
            "Python has robust libraries for building desktop interfaces:\n",
            "- **Tkinter**: Built-in Python library for simple GUIs.\n",
            "  ```python\n",
            "  import tkinter as tk\n",
            "  root = tk.Tk()\n",
            "  root.title(\"Hello World\")\n",
            "  label = tk.Label(root, text=\"Welcome to Python GUI!\")\n",
            "  label.pack()\n",
            "  root.mainloop()\n",
            "  ```\n",
            "- **PyQt/PySide**: Advanced frameworks for feature-rich desktop apps.\n",
            "- **Kivy**: Cross-platform framework (supports mobile apps too).\n",
            "\n",
            "---\n",
            "\n",
            "### **2. Web Frontends**\n",
            "While JavaScript dominates web frontends, Python offers alternatives:\n",
            "- **Brython**: Run Python in the browser (translates Python to JS).\n",
            "  ```html\n",
            "  <script type=\"text/python\">\n",
            "    from browser import document\n",
            "    document <= \"Hello from Python!\"\n",
            "  </script>\n",
            "  ```\n",
            "- **Anvil**: Full-stack framework with drag-and-drop UI designer.\n",
            "- **Streamlit/NiceGUI**: Build dashboards or data-focused web apps quickly.\n",
            "\n",
            "---\n",
            "\n",
            "### **3. Mobile Apps**\n",
            "- **Kivy**: Create cross-platform mobile apps with Python.\n",
            "- **BeeWare**: Tools like **Briefcase** and **Toga** for native mobile apps.\n",
            "\n",
            "---\n",
            "\n",
            "### **4. Hybrid Tools**\n",
            "- **PyScript**: Embed Python directly in HTML (experimental).\n",
            "  ```html\n",
            "  <py-script> print(\"Hello from PyScript!\") </py-script>\n",
            "  ```\n",
            "\n",
            "---\n",
            "\n",
            "### **Example with Streamlit (Web Dashboard)**\n",
            "Install: `pip install streamlit`\n",
            "```python\n",
            "import streamlit as st\n",
            "st.title(\"My Web App\")\n",
            "name = st.text_input(\"Enter your name\")\n",
            "st.write(f\"Hello, {name}!\")\n",
            "```\n",
            "Run with: `streamlit run app.py`\n",
            "\n",
            "---\n",
            "\n",
            "### **When to Use Python for Frontend?**\n",
            "- **Pros**: Quick prototyping, data-centric apps, or if you already know Python.\n",
            "- **Cons**: Not ideal for complex web UIs (JS frameworks like React/Vue are better).\n",
            "\n",
            "For most web projects, Python is better suited for the backend (with Flask/Django) while JavaScript handles the frontend. However, tools like **Anvil** or **Streamlit** let you build full-stack apps entirely in Python.\n"
          ]
        }
      ]
    }
  ]
}